---
title: "Notas de Aula - Capítulo 2"
subtitle: "Probabilidade"
author: "Caio Gomes Alves"
date: "`r format(Sys.Date(), '%d/%m/%Y')`"
header-includes:
   - \usepackage{amsmath}
   - \usepackage{tikz}
   - \usepackage{pgfplots}
   - \usepackage{caption}
   - \usepackage{subcaption}
   - \usepackage{cancel}
   - \usepackage{mathtools}
   - \usepackage{slashbox}
   - \usepackage{annotate-equations}
output:
  bookdown::pdf_document2:
    toc: false
---

# Variáveis Aleatórias

## Variáveis aleatórias e funções de distribuição

::: {.example}

Considere um experimento em que uma moeda é lançada duas vezes. Seja $X =$ total de caras nos dois lançamentos. Denotemos o evento cara como $H$ e coroa como $T$. Logo:

\begin{center}
\begin{tabular}{|c|c|} \toprule
Espaço Amostral ($\Omega$) & $X$ \\ \midrule
HT & 1 \\
TH & 1 \\
HH & 2 \\
TT & 0 \\ \bottomrule
\end{tabular}
\end{center}

Logo, $X:\mathcal{F} \to \mathbb{R}$. Vale também que, $\forall x$ valor na imagem de $X$, $X^{-1}(x) \in \mathcal{F}$. Por exemplo:

\begin{align*}
x = 1 &\Rightarrow X^{-1}(1) = \{HT,TH\} \\
x = 2 &\Rightarrow X^{-1}(2) = \{HH\} \\
x = 0 &\Rightarrow X^{-1}(0) = \{TT\} \\
\end{align*}

:::

::: {.definition name="Variável aleatória"}

Seja $(\Omega,\mathcal{F},P)$ um espaço de probabilidades. Uma função $X:\mathcal{F} \to \mathbb{R}$ é variável aleatória se $[x \in I] \in \mathcal{F}, \; I \in \mathbb{R}$ (ou, equivalentemente, se $\{\omega:X(\omega) \in I\} \in \mathcal{F}; \; X^{-1}(I) \in \mathcal{F}$).

:::

::: {.definition name="Distribuição Acumulada"}

Considere um espaço de probabilidades $(\Omega,\mathcal{F},P)$ e $X:\mathcal{F} \to \mathbb{R}$ uma variável aleatória, defina $F(r) = P(X \le r) = P(\{\omega : X(\omega) \le r\})$.

:::

::: {.example}

Seja $X =$ número de caras em dois lançamentos de moeda (honesta). Temos que as probabilidades de $X$ são dadas por:

\begin{align*}
P(X = 0) &= P(\{TT\}) = \frac{1}{4} \\
P(X = 1) &= P(\{TH,HT\}) = \frac{2}{4} \\
P(X = 2) &= P(\{HH\}) = \frac{1}{4}
\end{align*}

Para encontrarmos a função de distribuição acumulada, podemos particinar o espaço e "acumular" as probabilidades. Para $r < 0$:

\begin{equation*}
F(r) = P([X \le r]) = P(\emptyset) = 0
\end{equation*}

Para $r \in [0,1)$:

\begin{equation*}
F(r) = P([X \le r]) = P(X \le 0) = \frac{1}{4}
\end{equation*}

Para $r \in [1,2)$:

\begin{equation*}
F(r) = P([X \le r]) = P(X \le 1) = P(X = 0) + P(X = 1) = \frac{3}{4}
\end{equation*}

Para $r \ge 2$:

\begin{equation*}
F(r) = P([X \le r]) = P(X \le 2) = P(X = 0) + P(X = 1) + P(X = 2) = 1
\end{equation*}

Logo, $F$ é dada por:

\begin{equation*}
F(r) = \begin{cases} 0, & r<0 \\ \frac{1}{4}, & r \in [0,1) \\ \frac{3}{4}, & r \in [1,2) \\ 1, & r \ge 2 \end{cases}
\end{equation*}

\pgfplotsset{
  graf1/.style={
    xlabel=$X$,
    width=10cm, height=7cm,
    mark=*,
    nodes near coords,
    point meta=explicit symbolic, % permite usar a 3 coluna como label.
    every node near coord/.append style={font=\footnotesize},
    nodes near coords align={vertical}
  }
}

\pgfplotstableread{
  i    x   px   PX   f  F
  1    0 0.25 0.25 1/4 1/4
  2    1 0.50 0.75 2/4 3/4
  3    2 0.25 1.00 1/4 4/4
}\distrprob

\begin{center}
\begin{tikzpicture}[domain=0:2]
    \begin{axis}[
      graf1,
      grid=major,
      ytick distance=0.25,
      ymin=-0.1,
      ymax=1.2,
      ylabel={$P(X \le r)$},
      title={Distribui\c{c}\~{a}o de probabilidades acumulada}]
      \addplot[thick, black, const plot, jump mark left]
      table[x=x, y=PX, meta=F] \distrprob;
      \draw[black] (axis cs: -1, 0) -- (axis cs: 0, 0);
      \draw[black] (axis cs: 2, 1) -- (axis cs: 3, 1);
    \end{axis}
\end{tikzpicture}
\end{center}

:::

::: {.theorem name="Propriedades da distribuição acumulada"}

Seja $X$ uma variável aleatória definida em $(\Omega,\mathcal{F},P)$, então a f.d.a. de $X$ ($F_{X}$ ou $F$) verifica:

a) $F$ é monótona não decrescente;
b) $F$ é contínua à direita;
c) $\lim_{t \to - \infty}F(t) = 0$ e $\lim_{t \to \infty}F(t) = 1$.

:::

::: {.proof name="Prova"}

a) Dados $a,b \in \mathbb{R} : a \le b; \; [X \le a] \subseteq [X \le b] \Rightarrow P([X \le a]) \le P([X \le b]) \Rightarrow F(a) \le F(b)$.

b) Se $X_{n}\downarrow x$, quando $n \to \infty$, temos que $\{[X \le x_{n}]\}_{n \ge 1}$ é tal que $\bigcap_{n \ge 1}[X \le x_{n}] = [X \le x]$. Isso significa que $[X \le x]$ acontece se e somente se $[X \le x_{n}] \; \forall n$. Além disso, $[X \le x_{n}] \downarrow [X \le x]$ quando $n \to \infty$, logo, pela continuidade da função de probabilidade $P([X \le x_{n}]) \downarrow P([X \le x]), n \to \infty$.

c) Considere agora que $x_{n} \downarrow -\infty \Rightarrow [X \le x_{n}] \downarrow \emptyset, \; n \to \infty \Rightarrow F(x_{n}) = P([X \le x_{n}]) \downarrow P(\emptyset) = 0, \; n \to \infty$.
Se $x_{n} \uparrow \infty \Rightarrow [X \le x_{n}] \uparrow \Omega, \; n \to \infty \Rightarrow F(x_{n}) = P([X \le x_{n}]) \uparrow P(\Omega) = 1, \; n \to \infty$.

:::

::: {.theorem}

Se $F$ é a f.d.a. da variável aleatória $X$, então:

a) Existem e são finitos os limites laterais $\lim_{t \to r^{-}}F(t), \lim_{t \to r^{+}}F(t), \forall r \in \mathbb{R}$ e $\lim_{t \to r^{-}}F(t) \le \lim_{t \to r^{+}}F(t)$;

b) $\lim_{t \to r^{+}}F(t) = F(r), \forall r \in \mathbb{R}$;

c) $F$ é descontínua em $r, r \in \mathbb{R}$ se e somente se $\lim_{t \to r^{-}}F(t) < F(r)$, com um salto de tamanho $F(r) - \lim_{t \to r^{-}}F(t)$;

d) $\forall r \in \mathbb{R}, P(X = r) = F(r) - \lim_{t \to r^{-}}F(t)$;

e) Existem no máximo um total enumerável de descontinuidades em $F$.

:::

::: {.proof name="Prova"}

a) $F$ é monótona e limitada ($0 \le F \le 1$). Logo, os limites laterais existem e são limitados.

b) Como $F$ é monótona não-decrescente, $\forall x,y : x \le y \Rightarrow F(x) \le F(y)$. Logo $\lim_{t \to r^{-}}F(t) \le \lim_{t \to r^{+}}F(t)$.

c) Como $F$ é monótona não-decrescente, uma descontinuidade só ocorre se e somente se $\lim_{t \to r^{-}}F(t) < \lim_{t \to r^{+}}F(t) = F(r)$.

d) Seja $r \in \mathbb{R}. \; [X \le r] = \bigcap_{n=1}^{\infty}(r-\frac{1}{n} < x \le r)$, logo:

\begin{align*}
P([X = r]) &= P\left(\bigcap_{n=1}^{\infty}\left(r-\frac{1}{n} < x \le r\right)\right) \\
&\Downarrow \text{(Teorema da continuidade)} \\
&=\lim_{n \to \infty}P\left(\left(r - \frac{1}{n} < x \le r\right)\right) \\
&= \lim_{n \to \infty} \left(F(r) - F\left(r - \frac{1}{n}\right)\right) \\
&= F(r) - \lim_{n \to \infty}F\left(r - \frac{1}{n}\right) \\
P([X = r]) &= F(r) - \lim_{t \to r^{-}}F(t)
\end{align*}

e) Seja $\mathcal{D}$ o conjunto de pontos de descontinuidades de $F$, e seja $\lim_{t \to x^{-}}F(t) = F(x^{-})$. Logo:

\begin{equation*}
\mathcal{D} = \{x \in \mathbb{R} : F(x) - F(x^{-}) > 0\}
\end{equation*}

Seja $\mathcal{D}_{n}$ o conjunto de pontos para os quais a amplitude do salto é maior ou igual a $\frac{1}{n}$. Logo:

\begin{equation*}
\mathcal{D}_{n} = \left\{x \in \mathbb{R}: F(x) - F(x^{-}) \ge \frac{1}{n}\right\} \Rightarrow \# D = |D| \le n
\end{equation*}

Se $x \in \mathcal{D} \Rightarrow \exists n_{0} > 1 : F(x) - F(x^{-}) \ge \frac{1}{n_{0}} \Rightarrow x \in \bigcup_{n=1}^{\infty}\mathcal{D}_{n}$. Se $x \in \bigcup_{n=1}^{\infty}\mathcal{D}_{n} \Rightarrow \exists n_{1} : x \in \mathcal{D}_{n} \Rightarrow x \in \mathcal{D}$. $\mathcal{D}$ portanto é a união enumerável de conjuntos finitos, logo é enumerável.

:::

## Natureza das variáveis aleatórias

a) $X$ é uma variável aleatória discreta se os valores que ela toma pertencem a um conjunto enumerável, logo $X:\Omega \to \{x_{1},x_{2},\ldots\}$ (ou seja, $X(\omega) \in \{x_{1},x_{2},\ldots\}, \forall \omega \in \Omega$) e $P: \{x_{1},x_{2},\ldots\} \to [0,1]$ é dado por $P(x_{i}) = P\{\omega : \omega \in \Omega \text{ e } X(\omega) = x_{i}\} \forall i \ge 1$.

b) $X$ é uma variável aleatória absolutamente contínua se $\exists f$ (uma função) tal que $f(x) \ge 0, \forall x \in \mathbb{R}$ e $F_{X}(x) = \int_{-\infty}^{x}f(t) dt$ (onde $f$ é chamada de densidade de $X$).

Sob **(a)** temos que $[X \le x] = \bigcup_{i : x_{i} \le x} [X = x_{i}]$. Logo $F_{x}(x) = \sum_{i : x_{i} \le x}P(x_{i})$.

Sob **(b)** estamos afirmando que $F_{X}$ é a integral de $f$ (ou seja, $f$ é a sua derivada) para todo $x$ exceto em um conjunto de medida de Lebesgue nula, ou seja, se seu comprimento for zero ($\int_{a}^{a}f(t) dt = 0$). Ainda sob **(b)**, se $f$ é uma função de densidade podemos definir $F(x) = \int_{-\infty}^{x} f(t) dt$ e $F$ verifica:

1. $x \le y \Rightarrow F(x) \le F(y)$;
2. Se $x_{n} \downarrow x \Rightarrow F(x_{n}) \downarrow F(x)$;
3. Se $x_{n} \downarrow -\infty \Rightarrow F(x_{n}) \downarrow 0$ e se $x_{n}\uparrow \infty \Rightarrow F(x_{n}) \uparrow 1$.

Dada uma variável aleatória com distribuição $F_{X}$, $X$ tem densidade se:

(i) $F_{X}$ é contínua;
(ii) $F_{X}$ é derivável por partes (ou derivável no interior de um número finito ou enumerável de intervalos fechados cuja união é igual a $\mathbb{R}$), ou derivável para todo $x$ exceto um número finito (enumerável) de pontos.

::: {.example}

\pgfplotsset{
  graf2/.style={
    xlabel=$x$,
    ylabel=$P(X \le x)$,
    width=7cm, height=7cm,
    ymin=-0.1,
    ymax=1.2
  }
}

\begin{center}
\begin{tikzpicture}[domain=0:1.1]
    \begin{axis}[
        graf2,
        grid = major,
        ytick distance=0.25,
        xtick distance=0.25]
        \addplot[draw=none] {0};
        \draw[black] (axis cs: -1, 0) -- (axis cs: 3, 0);
        \draw[black] (axis cs: 0, -1) -- (axis cs: 0, 3);
        \draw[very thick, black] (axis cs: -1, 0) -- (axis cs: 0, 0);
        \draw[very thick, black] (axis cs: 0, 0) -- (axis cs: 1, 1) node[above left] {$F_{X}(x)$};
        \draw[very thick, black] (axis cs: 1, 1) -- (axis cs: 1.5,1);
    \end{axis}
    \node [shape=rectangle, align=center](equation1) at (-5,2.5) {$F_{X}(x) = \begin{cases} 0, & x<0 \\ x, & x \in [0,1] \\ 1, & x > 1 \end{cases}$};
\end{tikzpicture}
\end{center}

Notas:

- $F_{X}$ é contínua;
- $\{0,1\}$ são pontos sem derivada;
- Podemos definir os seguintes intervalos em que $F_{X}$ é derivável: $(-\infty,0),(0,1),(1,\infty)$;
- $F_{X}'(x) = \begin{cases}1, & x \in (0,1) = f_{X}(x) \\ 0 , & c.c.\end{cases}$;
- $f(0)$ e $f(1)$ podem ser definidos como zero ou um, já que tais definições não alteram $F_{X}(x) = \int_{-\infty}^{x}f(t)dt$.

Em contrapartida, considere:

\begin{center}
\begin{tikzpicture}[domain=0:1.1]
    \begin{axis}[
        graf2,
        grid = major,
        ytick distance=0.25,
        xtick distance=0.25]
        \addplot[draw=none] {0};
        \draw[black] (axis cs: -1, 0) -- (axis cs: 3, 0);
        \draw[black] (axis cs: 0, -1) -- (axis cs: 0, 3);
        \draw[very thick, black] (axis cs: -1, 0) -- (axis cs: 0, 0);
        \draw[very thick, black] (axis cs: 0, 1) -- (axis cs: 1, 1) node[above left] {$F_{X}(x)$};
        \draw[very thick, black] (axis cs: 1, 1) -- (axis cs: 1.5,1);
        \node[circle,fill=black,scale=0.4] (PA) at (axis cs: 0,1) {};
        \node[circle,fill=white,scale=0.4,draw] (P0) at (axis cs: 0,0) {};
    \end{axis}
    \node [shape=rectangle, align=center](equation1) at (-5,2.5) {$F_{X}(x) = \begin{cases} 0, & x<0 \\ 1, & x \ge 0 \end{cases}$};
\end{tikzpicture}
\end{center}

Notas:

- $F_{X}$ não é contínua;
- $P(X=0) = \lim_{x \to 0^{+}}F_{X}(x) - \lim_{x \to 0^{-}}F_{X}(x) = 1$.

:::

::: {.example}

Considere a densidade triangular:

\pgfplotsset{
  graf3/.style={
    xlabel=$x$,
    width=10cm, height=6cm,
    ymin=-0.1,
    ymax=1.5
  }
}

\begin{center}
\begin{tikzpicture}[domain=0:2.5]
  \begin{axis}[graf3, grid=major]
  \addplot[draw=none] {0};
  \draw[black] (axis cs: -1, 0) -- (axis cs: 3, 0);
  \draw[black] (axis cs: 0, -1) -- (axis cs: 0, 3);
  \draw[very thick, black] (axis cs: -1, 0) -- (axis cs: 0, 0);
  \draw[very thick, black] (axis cs: 0, 0) -- (axis cs: 1, 1);
  \draw[very thick, black] (axis cs: 1, 1) -- (axis cs: 2, 0) node[above right] {$f_{X}(x)$};
  \draw[very thick, black] (axis cs: 2, 0) -- (axis cs: 3, 0);
  \end{axis}
  \node [shape=rectangle, align=center](equation1) at (-3.5,2.5) {$f_{X}(x) = \begin{cases} x, & \text{se } 0 \le x < 1 \\ 2-x, & \text{se } 1 \le x < 2 \\ 0 & c.c. \end{cases}$};
\end{tikzpicture}
\end{center}

Por definição, $f(x) \ge 0 \; \forall x$. Para verificarmos que a probabilidade total é igual a um, podemos realizar a seguinte integração por partes:

\begin{align*}
\int_{-\infty}^{x}f_{X}(x)\mathrm{d}x &= \int_{0}^{2}f_{X}(x)\mathrm{d}x \\
&= \int_{0}^{1}x \mathrm{d}x + \int_{1}^{2}(2-x)\mathrm{d}x \\
&= \frac{x^{2}}{2}\Big{|}_{0}^{1} + 2x \Big{|}_{1}^{2} - \frac{x^{2}}{2} \Big{|}_{1}^{2} \\
&= 1
\end{align*}

O que demonstra que $f_{X}(x)$ é densidade de probabilidade.

:::

::: {.conjecture}

Cada função de distribuição se corresponde com apenas uma distribuição? Não.

:::

::: {.proof name="Prova"}

Considere, por exemplo, que a variável aleatória $X \sim N(0,1)$. Logo, a sua função distribuição de probabilidade é dada por $f_{X}(x) = \frac{1}{\sqrt{2\pi}}e^{-\frac{x^{2}}{2}}$ e $\Phi(x)$ é sua acumulada. Vejamos que $X \sim N(0,1) \Longleftrightarrow -X \sim N(0,1)$:

Seja $\omega$ um possível valor de $-X$, devemos calcular $P(-X \le \omega)$ e provar que $P(-X \le \omega) = \Phi(\omega)$:

\begin{equation*}
P(-X \le \omega) = P(X \ge -\omega) = 1 - P(X \le \omega) = 1 - \Phi(-\omega) = 1 - (1 - \Phi(\omega)) = \Phi(\omega)
\end{equation*}

:::

## Variáveis aleatórias e $\sigma$-álgebra de Borel

Se $X$ é uma variável aleatória em $(\Omega, \mathcal{A}, P)$, cada evento $[X \le x] \in \mathcal{A} \; \forall x \in \mathbb{R}$. Isto é, $[X \in \mathcal{B}]$, onde $[X \in \mathcal{B}] = [X \le x]$ é um evento e $P(X \in \mathcal{B})$ é bem definido. No entanto, a operacionalidade do sistema $(\Omega, \mathcal{A},P)$ pode ser estendido a todo boreliano (ou seja, a todos os elementos da $\sigma$-álgebra de Borel, que é a menor $\sigma$-álgebra contendo os intervalos cujos comprimentos estejam bem definidos).

::: {.proposition}

Se $X$ é uma variável aleatória em $(\Omega,\mathcal{A},P)$, então o evento $[x \in \mathcal{B}] = \{\omega : \omega \in \Omega \text{ e } X(\omega) \in \mathcal{B}\}$ é um evento aleatório para todo $\mathcal{B}$ boreliano (ou seja, $[x \in B] \in \mathcal{A} \; \forall B \in \mathcal{B}$).

:::

Podemos ver que diferentes tipos de intervalos (leia-se borelianos) podem ser mostrados como pertencentes à $\sigma$-álgebra, de modo que variáveis aleatórias que operam sobre esses intervalos estarão bem definidas:

1. Se $B = (-\infty, b] \Rightarrow [X \in B] \in \mathcal{A}$ de acordo com a definição de variável aleatória;
2. Se $B = (a, \infty)$, podemos fazer $B = (-\infty, a]^{c}$. Como o evento $[X \le a] \in \mathcal{A}$ por definição, sendo $\mathcal{A}$ uma $\sigma$-álgebra, deve ocorrer que $[X \le a]^{c} = B \in \mathcal{A}$, ou seja, $B \in \mathcal{A}$;
3. Se $B = (a, b] \Rightarrow [X \in B] = [X \in (a,b]] = [X \le b] - [X \le a]$. Como $[X \le b] \in \mathcal{A}$ e $[X \le a] \in \mathcal{A}$, então $P(X \in B) = P(X \le b) - P(x \le a) = F_{X}(b) - F_{X}(a)$;
4. Se $B = (a,b) \Rightarrow B = \bigcup_{n=1}^{\infty}\left(a,b-\frac{1}{n}\right]$ Sabemos que os eventos $\left(a < X \le b - \frac{1}{n}\right] \in \mathcal{A}$ e as suas uniões também pertencem à $\mathcal{A}$. Quanto à probabilidade, temos $P(X \in B) = P\left(\bigcup_{n=1}^{\infty}\left(a < X \le b - \frac{1}{n}\right]\right) = \lim_{n \to \infty}P\left(\left(a < X \le b - \frac{1}{n}\right]\right) = \lim_{n \to \infty}F_{X}\left(b - \frac{1}{n}\right) - F_{X}(a) = F_{X}(b^{-}) - F_{X}(a)$;
5. Se $B = \bigcup_{i = 1}^{n}B_{i}:B_{i} \in \mathcal{A}\; \forall i$, e sendo os $B_{i}$'s disjuntos, temos que $[X \in B] = \bigcup_{i=1}^{n}[X \in B_{i}] \Rightarrow P([X \in B]) = \sum_{i=1}^{n}P(X \in B_{i})$.

Podemos assim reformular os axiomas de Kolmogorov:

- $Ax_{1}(K)$: $P_{X}(B) = P(X \in B) \ge 0$;
- $Ax_{2}(K)$: $P_{X}(\mathbb{R}) = P(X \in \mathbb{R}) = 1$;
- $Ax_{3}(K)$: Se $B_{1}, \ldots, B_{n} \in \mathcal{B}$, com $B_{i} \cap B_{j} = \emptyset \; \forall i \neq j \Rightarrow P_{X}(\bigcup B_{n}) = P(X \in \bigcup_{n}B_{n}) = P(\bigcup_{n}[X \in B_{n}]) = \sum_{n}P(X \in B_{n})$.

::: {.definition}

A probabilidade $P_{X}$ definida na $\sigma$-álgebra de Borel por $P_{X}(B) = P(X \in B)$ é a distribuição de $X$.

:::

::: {.proposition}

a) Se $X$ é uma variável aleatória discreta com valores em $\{x_{1},x_{2},\ldots\} \Rightarrow P_{X}(B) = \sum_{i:x_{i} \in B}P(x_{i})$;
b) Se $X$ é absolutamente contínua com densidade $f \Rightarrow P_{X}(B) = \int_{B}f_{X}dx$.

:::

## Variáveis contínuas

::: {.proposition #transfsoma}

Se $X \sim f_{X}, \; y = bx+c, \; b>0$ e $c \in \mathbb{R} \Rightarrow Y \sim f_{Y}$ onde $f_{Y}(y) = \frac{1}{b}f_{X}(\frac{y-c}{b}); y \in \mathbb{R}$, onde $c$ é dito um parâmetro de posição (muitas vezes de posição central) e $b$ um parâmetro de escala.

:::

### Exemplos

::: {.example name="Distribuição Normal"}

\begin{equation*}
f(x) = \frac{1}{\sqrt{2\pi}}e^{-\frac{x^{2}}{2}} \Longrightarrow f_{\mu,\sigma}(x) = \frac{1}{\sqrt{2\pi\sigma^{2}}}e^{-\frac{(x-\mu)^{2}}{2 \sigma^{2}}}
\end{equation*}

Aqui, $\mu$ representa a média (posição central) da distribuição e $\sigma^{2}$ a sua variância.

:::

::: {.example name="Distribuição Cauchy"}

\begin{equation*}
f(x) = \frac{1}{\pi(1 + x^{2})} \Longrightarrow f_{b,M}(x) = \frac{1}{b}\frac{1}{\pi\left(1+\left(\frac{x-M}{b}\right)^{2}\right)} = \frac{b}{\pi(b^{2}+(x-M)^2)}
\end{equation*}

Neste caso, $M$ é a mediana da distribuição e $b$ representa a distância entre $M$ e o 1º quartil da distribuição.

:::

::: {.example name="Distribuições Exponencial e Gamma"}

Considere $g(x) = e^{-x} I_{0,\infty}(x)$. Sabemos que $g$ é uma distribuição de probabilidade pois:

\begin{equation*}
\begin{cases*}
g(x) \ge 0 \;\forall x \in (0,\infty) \\
\int_{0}^{\infty}e^{-x}dx = 1
\end{cases*}
\end{equation*}

Vamos agora incluir no formato do tipo exponencial um componente polinomial. Dado $\alpha > 0$, defina $g(x) = x^{\alpha - 1}e^{-x}$. Podemos ver que $g$ é integrável, de modo que:

\begin{align*}
\int_{0}^{\infty}g(x)dx &= \int_{0}^{\infty}x^{\alpha - 1}e^{-x}dx = \Gamma(\alpha) \\
f_{X}(x) &= \begin{cases}\frac{1}{\Gamma(\alpha)}x^{\alpha - 1}e^{-x} & x>0 \\ 0 & c.c.\end{cases}
\end{align*}

Defina agora $y = \frac{X}{\beta}$ onde $X \sim \text{Gamma}(\alpha,1)$ e $\beta > 0$. A densidade de $Y$ pode ser encontrada por meio de:

\begin{align*}
P(Y \le y) &= P\left(\frac{X}{\beta} \le y\right) = P(X \le \beta y) \Rightarrow F_{Y}(y) = F_{X}(\beta y) \\
f_{Y}(y) &= \beta f_{X}(\beta y) = \beta\frac{(\beta y)^{\alpha - 1}}{\Gamma(\alpha)}e^{-\beta y} = \frac{\beta^{\alpha}}{\Gamma(\alpha)}y^{\alpha - 1}e^{-\beta y}
\end{align*}

Nesse caso (conhecido como distribuição Gama) $\frac{1}{\beta}$ é um parâmetro de escala e $\alpha$ é um parâmetro de forma. Temos alguns casos especiais, como:

- Se $\alpha = 1\; : Y \sim \text{Exp}(\beta)$;
- Se $\alpha = \frac{n}{2}$, com $n$ inteiro e $\beta = \frac{1}{2} \;: Y \sim \chi^{2}(n)$

:::

## Variáveis aleatórias multidimensionais

::: {.definition}

A distribuição de probabilidades do vetor aleatório dado por $(x_{1},\ldots,x_{n})$ é uma tabela que associa a cada valor $(x_{1},\ldots,x_{n})$ sua probabilidade $P(x_{1},\ldots,x_{n}) = P(X_{1} = x_{1}, \ldots,X_{n}=x_{n})$, onde $p$ é a distribuição conjunta.

:::

::: {.example}

Considere o conjunto de 32 cartas para poker: 7,8,9,10,J,Q,K,A, dos 4 naipes. Duas cartas são retiradas aleatoriamente, sem reposição, e $X =$ número de ases que a pessoa recebe e $Y =$ número de cartas de copas que a pessoa recebe. Qual a probabilidade $P(X=0,Y=0)$?

\begin{equation*}
P(X=0,Y=0) = \frac{\binom{21}{2}}{\binom{32}{2}} = \frac{210}{496}
\end{equation*}

:::

::: {.definition}

A função de distribuição acumulada do par de variáve aleatórias $(X,Y)$ é dada por:

\begin{equation*}
F(X,Y) = P(X \le x, Y \le y) = \sum_{\{i:x_{i} \le x\}}\sum_{\{j: y_{j} \le y\}}P(X = x_{i}, Y = y_{i})
\end{equation*}

:::

Seja $\underbar{X} = (X_{1}, \ldots, X_{n})$ tal que $X_{i}$ é variável aleatória definida em $(\Omega, \mathcal{A}, P) \; \forall i$. Então $F$, a acumulada de $\underbar{X}$ verifica:

- **$F_{1}$**: $F$ é não decrescente em cada uma das coordenadas;
- **$F_{2}$**: $F$ é contínua à direita em cada uma das coordenadas;
- **$F_{3}$**: $\lim_{x_{i} \to -\infty}F(x_{1}, \ldots, x_{n}) = 0$ e $\lim_{x_{i} \to \infty \forall i}F(x_{1}, \ldots, x_{n}) = 1$.

As provas de $F_{1}$ e $F_{2}$ são de simples construção. Para $F_{3}$ temos:

::: {.proof name="Prova"}

Considere $i$ fixo e o evento $[X_{1} \le x_{1}, \ldots, X_{i-1} \le x_{i-1}, X_{i} \le -m, X_{i+1} \le x_{i+1}, \ldots, X_{n} \le x_{n}]$. Logo, $F(x_{1},\ldots,x_{i-1},-m,x_{i+1},\ldots,x_{n}) \xrightarrow[m \to \infty]{} 0$.

Por outro lado, note que $[X_{1} \le x_{1}, \ldots, X_{i-1} \le x_{i-1}, X_{i} \le m, X_{i+1} \le x_{i+1}, \ldots, X_{n} \le x_{n}] \xrightarrow[m \to \infty]{} [X_{1} \le x_{1}, \ldots, X_{i-1} \le x_{i-1}, X_{i+1} \le x_{i+1}, \ldots, X_{n} \le x_{n}]$ (que é o evento marginal sem o $X_{i}$). Já se $x_{i} \to \infty \; \forall i : \bigcap_{i=1}^{n}[X_{i} \le x_{i}] \uparrow \Omega \Rightarrow F(x_{1}, \ldots, x_{n}) = P\left(\bigcap_{i=1}^{n}[X_{i} \le x_{i}]\right) \uparrow 1, x_{i} \to \infty \; \forall i$.

:::

$F_{1},F_{2}$ e $F_{3}$ não são condições suficientes para que $F$ seja uma função de distribuição acumulada. Vejamos um exemplo que segue $F_{1},F_{2}$ e $F_{3}$ e que não é função de distribuição acumulada:

Seja $F_{0}(x,y) = \begin{cases}1 & \text{se }x \ge 0, y \ge 0, x + y \ge 1 \\ 0 & \text{c.c.} \end{cases}$. Graficamente, temos:

\pgfplotsset{
  graf4/.style={
    xlabel=$x$,
    ylabel=$y$,
    width=7cm, height=7cm,
    ymin=-0.1,
    ymax=1.3
  }
}

\begin{center}
\begin{tikzpicture}[domain=0:1.1]
    \begin{axis}[
        graf4,
        grid = major,
        ytick distance=0.5,
        xtick distance=0.5]
        \addplot[draw=none] {0};
        \draw[very thick, black] (axis cs: 0, 1) -- (axis cs: 1, 0);
        \draw[very thick, black] (axis cs: 0, 1) -- (axis cs: 0, 3);
        \draw[very thick, black] (axis cs: 1, 0) -- (axis cs: 3,0);
        \draw[thin, black] (axis cs: 0, 0) -- (axis cs: 0, 1);
        \draw[thin, black] (axis cs: 0, 0) -- (axis cs: 1, 0);
        \fill[fill=black!40!white, opacity=0.5] (axis cs: 0, 5) -- (axis cs: 0, 1) -- (axis cs: 1, 0) -- (axis cs: 5, 0);
        \fill[fill=black!20!white, opacity=0.5] (axis cs: 0, 0) -- (axis cs: 0, 1) -- (axis cs: 1, 0);
        \node at (axis cs: 0.25, 0.25) {$F(x,y) = 0$};
        \node at (axis cs: 0.75, 0.75) {$F(x,y) = 1$};
    \end{axis}
\end{tikzpicture}
\end{center}

É fácil ver que $F_{0}$ segue $F_{1}, F_{2}$ e $F_{3}$, mas vejamos que $F_{0}$ atribui probabilidade negativa a certos eventos, a ver $[0 \le X \le 1, 0 \le Y \le 1]$:

\begin{align*}
F_{0}(0,0) &= P(X \le 0, Y \le 0) \\
F_{0}(1,1) &= P(X \le 1, Y \le 1) \\
F_{0}(1,1) - F_{0}(1,0) &= P(X \le 1, Y \le 1) - P(X \le 1, Y \le 0) = P(X \le 1, 0 \le Y \le 1)\\
F_{0}(0,1) - F_{0}(0,0) &= P(X \le 0, Y \le 1) - P(X \le 0, Y \le 0) = P(X \le 0, 0 \le Y \le 1)\\
F_{0}(1,1) - F_{0}(1,0) - F_{0}(0,1) - F_{0}(0,0) &= P(X \le 1, 0 \le Y \le 1) - P(X \le 0, 0 \le Y \le 1) \\
&= P(0 \le X \le 1, 0 \le Y \le 1) = -1
\end{align*}

Defina $\Delta_{k,I}(g(x_{1},\ldots,x_{k})) = g(x_{1}, \ldots, x_{k-1},b) - g(x_{1}, \ldots, x_{k-1},a)$ onde $g:\mathbb{R}^{k} \to \mathbb{R}; I = (a,b], a \le b$. Logo, se $I_{1} = (a_{1},b_{1}]$ e $I_{2} = (a_{2},b_{2}], F:\mathbb{R}^{2} \to \mathbb{R}$. Então:

\begin{align*}
\Delta_{1,I_{1}}(\Delta_{2,I_{2}}(F(x,y))) &= \Delta_{1,I_{1}}(F(x, b_{2}) - F(x, a_{2})) \\
&= F(b_{1}, b_{2}) + F(a_{1}, a_{2}) - F(a_{1}, b_{2}) - F(b_{1}, a_{2}) \ge 0 \\
&= P(a_{1} < X \le b_{1}, a_{2} < Y \le b_{2}) \ge 0
\end{align*}

No geral:

- $F_{4}$: $\Delta_{1,I_{1}}\Delta_{2,I_{2}}\ldots\Delta_{n,I_{n}}(F(x_{1},\ldots,x_{n})) \ge 0 \; \forall I_{k} = (a_{k}, b_{k}]; a_{k} \le b_{k}, k=1,\ldots,n$.

::: {.definition}

Seja $F:\mathbb{R}^{n} \to \mathbb{R}$ seguindo $F_{1}, F_{2}, F_{3}$ e $F_{4}$, logo $F$ é uma função de distribuição acumulada n-dimensional (ou n-variada).

- **a)** Se o vetor aleatório $(X_{1},\ldots,X_{n})$ toma valores em um conjunto discreto, o vetor é discreto;
- **b)** Se para o vetor aleatório $(X_{1},\ldots,X_{n})$, $F$ é dada pela forma $F(x_{1}, \ldots, x_{n}) = \int_{-\infty}^{x_{n}} \ldots \int_{-\infty}^{x_{1}}f(t_{1}, \ldots, t_{n}) dt_{n} \ldots dt_{1}, \\ \forall (x_{1},\ldots, x_{n})$ onde $f(t_{1}, \ldots, t_{n}) \ge 0 \; \forall (t_{1},\ldots,t_{n}) \in \mathbb{R}^{n}$ então $(X_{1},\ldots,X_{n})$ é um vetor absolutamente contínuo com densidade $f$ (densidade conjunta).

:::

::: {.definition}

A probabilidade definida em $\mathcal{B}^{n}$ (borelianos em $\mathbb{R}^{n}$) por $P(\underbar{X} \in B)$ (com $B \in \mathcal{B}^{n}$) é chamada de distribuição conjunta de $\underbar{X} = (X_{1},\ldots,X_{n})$, com notação: $P_{\underbar{X}}(B) = P(\underbar{X} \in B)$.

:::

::: {.proposition}

- **a)** Se o vetor aleatório $\underbar{X}$ é discreto, $P_{\underbar{X}}(B) = \sum_{\{i:x_{i} \in B\}}P(X_{i} = x_{i}) \; \forall B \in \mathcal{B}^{n}$;
- **b)** Se $\underbar{X}$ é absolutamente contínuo com densidade $f$, $P_{\underbar{X}}(B) = P(\underbar{X} \in B) = \int \ldots \int_{B} f(x_{1}, \ldots, x_{n}) dx_{n} \ldots dx_{1}$.

:::

## Independência

::: {.definition}

As variáveis aleatórias são (coletivamente) independentes se:

\begin{equation*}
P(X_{1} \in B_{1}, \ldots, X_{n} \in B_{n}) = \prod_{i=1}^{n}P(X_{i} \in B_{i}), \; \forall B_{i} \in \mathcal{B}^{n}, \forall i = 1, \ldots, n
\end{equation*}

:::

Se $X_{1},\ldots,X_{n}$ são coletivamente independentes, então $X_{i1},\ldots,X_{ik}$ são coletivamente independentes $\forall k$.

### Critérios ou consequências

::: {.proposition #propacumulada}

- **a)** Se $X_{1},\ldots,X_{n}$ são independentes, então $F_{X_{1} \ldots X_{n}}(x_{1},\ldots,x_{n}) = \prod_{i=1}^{n}F_{X_{i}}(x_{i}),\forall (x_{1},\ldots,x_{n}) \in \mathbb{R}^{n}$;
- **b)** Se existem funções $F_{1},\ldots,F_{n}$ tais que $\lim_{n \to \infty}F_{i}(x) = 1, \forall i$ e $F_{X_{1},\ldots,X_{n}}(x_{1},\ldots,x_{n}) = \prod_{i=1}^{n}F_{i}(x_{i}), \forall (x_{1},\ldots,x_{n}) \in \mathbb{R}^{n} \Rightarrow X_{1},\ldots, X_{n}$ são independentes e $F_{i} = F_{X_{i}}, \forall i$.

:::

::: {.proof name="Prova"}

- **a)** Se $X_{1},\ldots,X_{n}$ são coletivamente independentes e tomamos $[X_{i} \le x_{i}] = (-\infty, x_{i}] = B_{i}$. Então:

\begin{align*}
F_{X_{1} \ldots X_{n}}(x_{1},\ldots,x_{n}) &= P(X_{1} \le x_{1}, \ldots, X_{n} \le x_{n}) \\
&= P(X_{1} \in B_{1},\ldots, X_{n} \in B_{n}) \\
&\stackrel{Ind}{=} \prod_{i=1}^{n}P(X_{i} \in B_{i}) \\
&= \prod_{i=1}^{n}P(X_{i} \le x_{i}) = \prod_{i=1}^{n}F_{X_{i}}(x_{i}) \; \forall (x_{1},\ldots,x_{n})
\end{align*}

- **b)** Para cada $i$, $F_{X_{i}}(x_{i}) = P(X_{i} \le x_{i}) = \lim_{m \to \infty}P(X_{1} \le m, \ldots, X_{i-1} \le m, X_{i} \le x_{i}, X_{i+1} \le m, \ldots, X_{n} \le m)$, de modo que:

\begin{align*}
F_{X_{i}}(x_{i}) &= \lim_{m \to \infty}F_{X_{1}\ldots X_{n}}(m,\ldots,m,x_{i},m,\ldots,m) \\
&\stackrel{Hip}{=} \lim_{m \to \infty}\left(\prod_{j=1}^{i-1}F_{j}(m) \times F_{i}(x_{i}) \times \prod_{j=i+1}^{n}F_{j}(m)\right) \\
&= F_{i}(x_i)
\end{align*}

Logo, a marginal de $X_{i}$ é precisamente $F_{i}, \forall i$. Devemos ainda verificar que $P(X_{1} \in B_{1}, \ldots, X_{n} \in B_{n}) = \prod_{i=1}^{n}P(X_{i} \in B_{i}) \; \forall B_{i} \in \mathcal{B}^{n}$. Considere $B_{i} = (a_{i},b_{i}], a_{i} \le b_{i}, a_{i}, b_{i} \in \mathbb{R}$. Temos que:

\begin{align*}
P(X_{1} \in B_{1}, \ldots, X_{n} \in B_{n}) &= P(a_{1} < X_{1} \le b_{1}, \ldots, a_{n} < X_{n} \le b_{n}) \\
&= \Delta_{1,I_{1}} \ldots \Delta_{n,I_{n}}\left(F_{X_{1} \ldots X_{n}}(x_{1},\ldots,x_{n})\right) \\
&\stackrel{Ind}{=} \Delta_{1,I_{1}} \ldots \Delta_{n,I_{n}}(F_{X_{1}}(x_{1})\ldots F_{X_{n}}(x_{n})) \\
&= [F_{X_{1}}(b_{1}) - F_{X_{1}}(a_{1})] \times \ldots \times [F_{X_{n}}(b_{n}) - F_{X_{n}}(a_{n})] \\
&= \prod_{i=1}^{n}P(a_{i} < X_{i} \le b_{i}) = \prod_{i=1}^{n}P(X_{i} \in B_{i})
\end{align*}

:::

### Caso contínuo

::: {.proposition}

- **a)** Se $X_{1},\ldots,X_{n}$ são independentes e possuem densidades $f_{X_{1}}, \ldots, f_{X_{n}}$, respectivamente, então $f_{X_{1}\ldots X_{n}}(x_{1},\ldots,x_{n}) = \prod_{i=1}^{n}f_{X_{i}}(x_{i}) \; \forall (x_{1},\ldots,x_{n}) \in \mathbb{R}^{n}$ é a densidade conjunta de $X_{1},\ldots,X_{n}$;
- **b)** Se $X_{1},\ldots,X_{n}$ tem densidade conjunta $f_{X_{1}\ldots X_{n}}(x_{1},\ldots,x_{n}) : f_{X_{1}\ldots X_{n}}(x_{1},\ldots,x_{n}) = \prod_{i=1}^{n}f_{i}(x_{i}) \; \forall (x_{1},\ldots,x_{n}) \in \mathbb{R}^{n}$, onde $f_{i}(x) \ge 0 \; \forall x : \int_{-\infty}^{\infty}f_{i}(x)dx = 1 \; \forall i$, então $X_{1},\ldots,X_{n}$ são independentes e $f_{i}$ é a densidade marginal de $X_{i} \; \forall i$.

:::

::: {.proof name="Prova"}

- **a)** Como consequência da proposição \@ref(prp:propacumulada), temos que: $F_{X_{1}\ldots X_{n}}(x_{1},\ldots,x_{n}) = \prod_{i=1}^{n}F_{X_{i}}(x_{i}), \forall (x_{1},\ldots,x_{n})$. Logo, por definição temos:

\begin{equation*}
\prod_{i=1}^{n}F_{X_{i}}(x_{i}) = \prod_{i=1}^{n}\int_{-\infty}^{x_{i}}f_{X_{i}}(t)dt = \int_{-\infty}^{x_{1}}\dots\int_{-\infty}^{x_{n}}f_{X_{1}}(t_{1})\ldots f_{X_{n}}(t_{n}) dt_{n} \ldots dt_{1}
\end{equation*}

Assim, $f_{X_{1}},\ldots,f_{X_{n}}$ é a densidade conjunta.

- **b)** Considere:

\begin{align*}
F_{X_{1}\ldots X_{n}}(x_{1},\ldots,x_{n}) &= \int_{-\infty}^{x_{1}} \dots \int_{-\infty}^{x_{n}}f_{X_{1}\ldots X_{n}}(t_{1},\ldots,t_{n})dt_{n}\ldots dt_{1} \\
&= \int_{-\infty}^{x_{1}} \dots \int_{-\infty}^{x_{n}}f_{1}(t_{1}) \ldots f_{n}(t_{n}) dt_{n} \ldots dt_{1} \\
&= \prod_{i=1}^{n}\int_{-\infty}^{x_{i}}f_{i}(t_{i})dt_{i}
\end{align*}

Defina $F_{i}(x) = \int_{-\infty}^{x_{i}}f_{i}(t)dt$. Sendo assim:

\begin{equation*}
\prod_{i=1}^{n} \int_{-\infty}^{x_{i}}f_{i}(t_{i})dt_{i} = \prod_{i=1}^{n}F_{i}(x_{i})
\end{equation*}

Note que, pela hipótese nas $f_{i}$'s, as $F_{i}$'s são acumuladas em particular, e $F_{i}(x) \to 1, x \to \infty$, e pela proposição \@ref(prp:propacumulada): $F_{i}(x) = F_{X_{i}}(x_{i})$, logo $f_{X_{i}} = f_{i}$.

:::

### Propriedades

- **a)** Se $F(x,y)$ é a função de distribuição acumulada conjunta de $(X,Y)$, então $F_{X}(x) = \lim_{y \to \infty}F(x,y) = F(x,\infty)$ é a função de distribuição acumulada marginal de $X$;
- **b)** Se $f(x,y)$ é a função de densidade conjunta de $(X,Y)$, então $f_{X}(x) = \int_{-\infty}^{\infty}f(x,y)dy$ é a densidade marginal de $X$.

::: {.example}

\begin{equation*}
f_{XY}(x,y) = \frac{1}{2\pi \sigma_{1}\sigma_{2}\sqrt{1-\rho^{2}}}\exp\left\{-\frac{1}{2(1-\rho^{2})}\left[\left(\frac{x-\mu_{1}}{\sigma_{1}}\right)^{2} + \left(\frac{y-\mu_{2}}{\sigma_{2}}\right)^{2} - 2\rho \left(\frac{x-\mu_{1}}{\sigma_{1}}\right)\left(\frac{y-\mu_{2}}{\sigma_{2}}\right)\right]\right\}
\end{equation*}

Sendo $\sigma_{i} > 0, i = 1,2; -1 < \rho < 1; \mu_{i} \in \mathbb{R}, i = 1,2$. Logo, $(X,Y) \sim N_{2}\left(\binom{\mu_{1}}{\mu_{2}},\begin{bmatrix}\sigma_{1} & \rho \\ \rho & \sigma_{2}\end{bmatrix}\right)$, onde, caso $\rho = 0, X$ e $Y$ são independentes.

:::

## Distribuições de funções de vetores

Seja $\underbar{X} = (X_{1},\ldots,X_{n})$ um vetor aleatório em $(\Omega, \mathcal{A},P)$. Seja $Y = g(X_{1},\ldots,X_{n})$. Qual a distribuição de $Y$?

- **Nota 1**: Para que $Y$ seja variável aleatória cada $B \in \mathcal{B}$ é necessário que $g^{-1}(B)$ seja mensurável, ou seja:

\begin{align*}
g^{-1}(B) &= \{x:g(x) \in B\} \\
&\Downarrow \\
F_{Y}(y) &= P(g(x) \le y)
\end{align*}

Generalizando, se $Y = g(X_{1},\ldots,X_{n})$:

\begin{equation*}
F_{Y}(y) = P(g(X_{1},\ldots,X_{n}) \le y) = P((X_{1},\ldots,X_{n}) \in B_{y}) = P_{\underbar{X}}(B_{y})
\end{equation*}

Onde $B_{y} = \{(x_{1},\ldots,x_{n}):g(x_{1},\ldots,x_{n}) \le y\}$.

- **Nota 2**: Se $\underbar{X}$ for discreto:

\begin{equation*}
P_{Y}(y_{j}) = \sum_{\{i:g(x_{i}) = y_{j}\}}P_{\underbar{X}}(x_{i})
\end{equation*}

::: {.example}

Sejam $X \sim U(0,1)$ e $Y = -\ln(x)$. Temos que $\forall x$ valor de $X:x \in (-\infty,0] \cup [1,\infty)$ o valor de $f_{X}(x) = 0$. Seja $x \in (0,1) \Leftrightarrow -\ln(x) \in (0,\infty)$, logo $\forall y$ valor de $Y:y \in (0,\infty)$. Calculemos $F_{Y}(y) = P(Y \le y)$:

\begin{align*}
F_{Y}(y) &= P(Y \le y) = P(-\ln(X) \le y) \\
&= P(\ln(X) \ge -y) \\
&= P(X \ge e^{-y}) \\
&= 1-P(X < e^{-y}) = 1-e^{-y}
\end{align*}

Assim, temos que $Y \sim Exp(1)$.

:::

::: {.example}

Sejam $X \perp Y; X \sim U(0,1); Y \sim U(0,1); Z = \frac{X}{Y}$. Determinar a distribuição de $Z$:

Os valores que geram indefinição de $Z$ são: $X=Y=0$ e $Y=0,X>0$, assim a boa definição de $Z$ é no espaço $[0 < X \le 1, 0 < Y \le 1]$. Vejamos se esse intervalo contém toda a massa de probabilidade:

\begin{equation*}
P([0 < X \le 1, 0 < Y \le 1]) = P(0 < X \le 1) \times P(0 < Y \le 1) = 1 \times 1 = 1
\end{equation*}

Logo, basta avaliar o conjunto $[0 < X \le 1, 0 < Y \le 1] \Rightarrow [Z \in (0, \infty)]$. Assim, calculemos $F_{Z}(z)$:

\begin{equation*}
F_{Z}(z) = P(Z \le z) = P\left(\frac{X}{Y} \le z\right) \Rightarrow \left[\frac{X}{Y} \le z\right] = \left[X \le zY\right] = \left[\frac{X}{z} \le Y\right]
\end{equation*}

Sabemos que $X$ e $Y$ pertencem ao intervalo $(0,1] \times (0,1]$, de modo que temos duas regiões genéricas para explorar: $z<1$ e $z>1$. De maneira gráfica, temos as seguintes regiões (considere $c > 1$):

\begin{center}
\begin{tikzpicture}[domain=0:1.2]
    \begin{axis}[
        graf4,
        grid = major,
        ytick distance=0.5,
        xtick distance=0.5]
        \addplot[draw=none] {0};
        \draw[very thick, black] (axis cs: 0, 0) -- (axis cs: 2, 2);
        \draw[very thick, black] (axis cs: 0, 0) -- (axis cs: 2, 1);
        \draw[very thick, black] (axis cs: 0, 0) -- (axis cs: 1, 2);
        \draw[thick, black] (axis cs: 0, 0) -- (axis cs: 2, 0);
        \draw[thick, black] (axis cs: 0, 0) -- (axis cs: 0, 2);
        \draw[thick, black] (axis cs: 1, 0) -- (axis cs: 1, 1);
        \draw[thick, black] (axis cs: 0, 1) -- (axis cs: 1, 1);
        \fill[fill=black!60!blue, opacity=0.5] (axis cs: 0, 0) -- (axis cs: 1, 1) -- (axis cs: 1, 0);
        \fill[fill=black!60!green, opacity=0.5] (axis cs: 0, 0) -- (axis cs: 1, 1) -- (axis cs: 0, 1);
        \node at (axis cs: 1, 1.15) {$y=x$};
        \node at (axis cs: 1.15, 0.4) {$y = \frac{x}{c}$};
        \node at (axis cs: 0.4, 1.15) {$y = cx$};
    \end{axis}
\end{tikzpicture}
\end{center}

Podemos ver que a região azul corresponde aos casos onde $z > 1$ e a região verde corresponde aos casos onde $z < 1$. Assim:

- $z<1$: \begin{equation*}F_{Z}(z) = \int_{0}^{z}\int_{0}^{\frac{x}{z}}dydx = \int_{0}^{z}y\big{|}_{0}^{\frac{x}{z}}dx = \int_{0}^{z}\frac{x}{z}dx = \frac{1}{z} \times \frac{x^{2}}{2}\bigg{|}_{0}^{z} = \frac{z^{2}}{2z} = \frac{z}{2}\end{equation*}
- $z>1$: \begin{equation*}F_{Z}(z) = 1 - \frac{1}{2z}\end{equation*}

De modo que a distribuição acumulada de $Z$ é dada por:

\begin{equation*}
F_{Z}(z) = \begin{cases}
0 & ,z \in (-\infty,0] \\
\frac{z}{2} & ,z \in (0,1) \\
1-\frac{1}{2z} & ,z \in [1,\infty)
\end{cases}
\end{equation*}

Assim, $F_{Z}(z) = P\left(\frac{X}{Y} \le z \right) = P((X,Y) \in B_{z})$, onde os conjuntos $B_{z}$ podem ter formatos diferentes dependendo de $z$. A densidade será dada pela derivada de $F_{Z}(z)$ com relação a $z$:

\begin{equation*}
f_{Z}(z) = \begin{cases}
0 & ,z \le 0 \\
\frac{1}{2} & ,z \in (0,1) \\
\frac{1}{2z^{2}} & ,z \ge 1
\end{cases}
\end{equation*}

:::

### Distribuição da Soma

::: {.proposition #denssoma}

- **a)** Se $X$ e $Y$ tem densidade conjunta $f(x,y) \Rightarrow f_{X+Y}(z) = \int_{-\infty}^{\infty}f(z-t,t)dt = \int_{-\infty}^{\infty}f(t,z-t)dt$;
- **b)** Se $X \perp Y$ e $f_{X}$ e $f_{Y}$ são suas marginais, então $f_{X+Y}(z) = \int_{-\infty}^{\infty}f_{X}(z-t)f_{Y}(t)dt = \int_{-\infty}^{\infty}f_{X}(t)f_{Y}(z-t)dt$.

:::

::: {.proof name="Prova"}

Seja $Z = X+Y \Rightarrow [Z \le z] = [X+Y \le z] = [(x,y) \in B_{z}]$. Considerando $B_{z} = \{(x,y):x+y \le z\} = \{(x,y):x \le z-y\}$, temos que:

\begin{equation*}
F_{Z}(z) = \int\int_{B_{z}}f(x,y)dxdy = \int_{-\infty}^{\infty}\int_{-\infty}^{z-y}f(x,y)dxdy
\end{equation*}

Seja $y$ um valor fixo e defina $s=x+y, ds = dx$. Quando $x=z-y \Rightarrow s=z$, temos:

\begin{equation*}
F_{Z}(z) = \int_{-\infty}^{\infty}\int_{-\infty}^{z}f(s-y,y)dsdy = \int_{-\infty}^{z}\int_{-\infty}^{\infty}f(s-y,y)dyds = \int_{-\infty}^{z}g(s)ds
\end{equation*}

E $g$ é a densidade de $X+Y$, ou seja, $g(s) = f_{X+Y}(s)$.

:::

### Convolução

Se $f_{1}$ e $f_{2}$ são densidades de variáveis aleatórias, sua convolução $f_{1}*f_{2}$ é:

\begin{equation*}
f_{1}*f_{2}(x) = \int_{-\infty}^{\infty}f_{1}(x-t)f_{2}(t)dt
\end{equation*}

Assim, no caso da soma da proposição \@ref(prp:denssoma), podemos ver que:

\begin{equation*}
f_{X+Y}(z) = f_{X}*f_{Y}(z)
\end{equation*}

### Independência

::: {.proposition}

Se $X_{1}, \ldots, X_{n}$ são variáveis aleatórias independentes, então funções de famílias disjuntas de $\{X_{i}\}_{i \ge 1}$ também são independentes.

:::

::: {.proof name="Prova: Caso especial"}

Considere $Y_{i} = g_{i}(X_{i})$. É necessário provar que $F_{Y_{1} \ldots Y_{n}}(y_{1},\ldots,y_{n}) = \prod_{i=1}^{n}F_{Y_{i}}(y_{i})$:

\begin{align*}
F_{Y_{1} \ldots Y_{n}}(y_{1},\ldots,y_{n}) &= P(g_{1}(x_{1}) \le y_{1}, \ldots, g_{n}(x_{n}) \le y_{n}) \\
&= P\left(X_{1} \in g_{1}^{-1}((-\infty, y_{1}]),\ldots,X_{n} \in g_{n}^{-1}((-\infty, y_{n}])\right) \\
&= \prod_{i=1}^{n}P\left(X_{i} \in g_{i}^{-1}((-\infty,y_{i}])\right) \\
&= \prod_{i=1}^{n}P\left(g_{i}(X_{i}) \in (-\infty,y_{i}]\right) = \prod_{i=1}^{n}F_{Y_{i}}(y_{i})
\end{align*}

:::

::: {.example #motivacaoexponencial}

Considere $X \perp Y, \; X \sim Exp(1)$ e $Y \sim Exp(1)$. Determine:

- a) A distribuição de $Z = X + Y$ e $W = \frac{X}{Y}$;
- b) Mostrar que $Z \perp W$.

**a)**

Como os valores de $X$ e $Y$ são sempre positivos, os valores de $Z$ e $W$ também o serão. Verifiquemos que $F_{ZW}(z,w) = F_{Z}(z)F_{W}(w)$:

\begin{align*}
P[Z \le z, W \le w] &= F_{ZW}(z,w) \\
&= \left[X+Y \le z, \frac{X}{Y} \le w\right] \\
&= \left[Y \le z - X, \frac{X}{w} \le Y\right]
\end{align*}

Vejamos que temos que considerar que $Y \le z-X$ e que $\frac{X}{w} \le Y$, ou seja, temos que avaliar as variáveis no seguinte boreliano:

\begin{center}
\begin{tikzpicture}
  \draw[very thick, ->] (-1,0) -- (4,0);
  \draw[very thick, ->] (0,-1) -- (0,4);
  \node at (4,-0.2) {x};
  \node at (-0.2,4) {y};
  \node at (3,-0.2) {z};
  \node at (-0.2,3) {z};
  \draw[thick] (0,3) -- (3,0);
  \fill[fill=black!60!white, opacity=0.5] (0, 0) -- (3, 0) -- (0, 3);
  \draw[thick] (0,0) -- (4,4);
  \node at (4.2,4.2) {$\frac{X}{w}$};
  \fill[fill=white!60!blue, opacity=0.5] (0, 0) -- (4, 4) -- (0, 4);
  \node at (-1.9,1.5) {$\mathcal{B}_{z,w}$};
  \draw[thick, <-] (0.75,1.5) -- (-1.5,1.5);
  \node at (1.5,-0.2) {p};
  \draw[thick,dotted] (1.5,0) -- (1.5,1.5);
\end{tikzpicture}
\end{center}

Onde a região em azul claro são os valores onde $Y \ge \frac{X}{w}$, e a região cinza são os valores em que $Y \le z - X$, o ponto $p$ é dado por:

\begin{align*}
\frac{X}{w} = z - X \Rightarrow z &= X \left(\frac{1}{w} +1\right) \\
z &= X\left(\frac{w+1}{w}\right) \\
X &= \frac{zw}{w+1}
\end{align*}

Assim, estamos interessados em encontrar $P((X,Y) \in \mathcal{B}_{z,w})$, que será:

\begin{align*}
P((X,Y) \in \mathcal{B}_{z,w}) &= \int_{0}^{p}\int_{\frac{x}{w}}^{z-x}e^{-x}e^{-y}dydx \\
&= \int_{0}^{\frac{zw}{w+1}}e^{-x}\left[-e^{-y}\big{|}_{\frac{x}{w}}^{z-x}\right]dx \\
&= \int_{0}^{\frac{zw}{w+1}}e^{-x}\left[e^{-\frac{x}{w}-e^{-z+x}}\right]dx \\
&= \int_{0}^{\frac{zw}{w+1}}e^{-x\left(\frac{1 + w}{w}\right)}-e^{-z}dx \\
&= -\frac{w}{(1+w)}e^{-x\left(\frac{1 + w}{w}\right)}\bigg{|}_{0}^{\frac{zw}{w+1}}-e^{-z}x\bigg{|}_{0}^{\frac{zw}{w+1}} \\
&= \frac{w}{1+w}\left(1-e^{-z}-ze^{-z}\right)
\end{align*}

Assim, temos que a distribuição de $Z$ e $W$ será dada por:

\begin{equation*}
F_{ZW}(z,w) = \begin{cases}
0 & ,z \le 0, w \le 0 \\
\frac{w}{1+w}\left(1-e^{-z}-ze^{-z}\right) & ,z > 0, w > 0
\end{cases}
\end{equation*}

Que é uma distribuição de probabilidade, pois é absolutamente contínua (e por consequência, contínua à direita) e os seguintes limites são bem definidos:

\begin{align*}
\lim_{w \to 0}F_{ZW}(z,w) &= 0 \\
\lim_{z \to 0}F_{ZW}(z,w) &= 0 \\
\lim_{z \to \infty, w \to \infty}F_{ZW}(z,w) &= 1
\end{align*}

**b)**

Temos que as distribuições marginais de $Z$ e $W$ serão:

\begin{align*}
F_{Z}(z) &= \lim_{w \to \infty}F_{ZW}(z,w) = 1-e^{-z}-ze^{-z} \\
F_{W}(w) &= \lim_{z \to \infty}F_{ZW}(z,w) = \frac{w}{1+w}
\end{align*}

E como a distribuição conjunta é o produto das marginais, temos que $Z \perp W$. As densidades serão dadas pelas derivadas da distribuição acumulada conjunta, ou seja:

\begin{align*}
f_{ZW}(z,w) &= \frac{\partial}{\partial z}\frac{\partial}{\partial w}\left(\frac{w}{1+w}\left(1-e^{-z}-ze^{-z}\right)\right) \\
&= \frac{1}{(1+w)^{2}}ze^{-z}I_{(0,\infty)}(z)I_{(0,\infty)}(w)
\end{align*}

:::

## Método do Jacobiano

Seja $g:G_{0} \to G$, com $G,G_{0} \subseteq \mathbb{R}^{n}$ e ambos abertos. Então $g(x_{1},\ldots,x_{n}) = \left(g_{1}(x_{1},\ldots,x_{n}),\ldots,g_{n}(x_{1},\ldots,x_{n})\right) = (y_{1},\ldots,y_{n})$, com $g$ sendo bijetiva, ou seja, para todo $y$ valor de $Y$, existe $\underbar{x}$ valor de $X$ tal que $g(\underbar{x}) = y$.

Logo $g$ admite inversa usual $g^{-1} = h$, com $h = (h_{1}, \ldots, h_{n})$:

\begin{align*}
&x_{1} = h_{1}(y_{1},\ldots,y_{n}) \\
&\vdots \\
&x_{n} = h_{n}(y_{1},\ldots,y_{n})
\end{align*}

Vamos supor que existem as derivadas parciais $\frac{\partial x_{i}}{\partial y_{j}}, \forall i, \forall j$, e que elas são contínuas em $G$. Desejamos computar: $\int \ldots \int_{C}f_{Y}(y)dy$ ,em termos de $\int \ldots \int_{D}f_{X}(x)dx$.

::: {.example}

Sejam $Y = (Y_{1},Y_{2}) = \left(X_{1} + X_{2}, \frac{X_{1}}{X_{2}}\right)$. Teremos então que: $y_{1} = g_{1}(x_{1},x_{2}) = x_{1} + x_{2}$ e $y_{2} = g_{2}(x_{1},x_{2}) = \frac{x_{1}}{x_{2}}$. Temos assim os valores dos $y$'s em termos dos $x$'s, e desejamos encontrar o contrário:

\begin{align*}
y_{1} &= x_{1} + x_{2} \Rightarrow x_{1} = y_{1} - x_{2} \\
y_{2} &= \frac{y_{1}-x_{2}}{x_{2}} \Rightarrow x_{2} = \frac{y_{1}}{y_{2}+1} \Rightarrow x_{1} = \frac{y_{1}y_{2}}{y_{2} + 1}
\end{align*}

Agora que temos os valores de $X_{1}$ e $X_{2}$ em função de $Y_{1}$ e $Y_{2}$. Agora, podemos calcular as derivadas parciais de $x$ com relação a $y$:

\begin{align*}
\frac{\partial x_{1}}{\partial y_{1}} &= y_{2}(y_{2} + 1)^{-1} \\
\frac{\partial x_{1}}{\partial y_{2}} &= y_{1}(y_{2} + 1)^{-2} \\
\frac{\partial x_{2}}{\partial y_{1}} &= (y_{2} + 1)^{-1} \\
\frac{\partial x_{2}}{\partial y_{2}} &= -y_{1}(y_{2} + 1)^{-2}
\end{align*}

Definimos agora o Jacobiano:

\begin{equation}
J(\underbar{x},\underbar{y}) = \det \begin{bmatrix}
\frac{\partial x_{1}}{\partial y_{1}} & \frac{\partial x_{1}}{\partial y_{2}} & \hdots & \frac{\partial x_{1}}{\partial y_{n}} \\
\frac{\partial x_{2}}{\partial y_{1}} & \frac{\partial x_{2}}{\partial y_{2}} & \hdots & \frac{\partial x_{2}}{\partial y_{n}} \\
\vdots & \vdots  & \ddots & \vdots \\
\frac{\partial x_{n}}{\partial y_{1}} & \frac{\partial x_{n}}{\partial y_{2}} & \hdots & \frac{\partial x_{n}}{\partial y_{n}}
\end{bmatrix}
(\#eq:jacobiano)
\end{equation}

Dessa forma, o Jacobiano da transformação será:

\begin{align*}
J(\underbar{x},\underbar{y}) &= \det \begin{bmatrix}
y_{2}(y_{2} + 1)^{-1} & y_{1}(y_{2} + 1)^{-2} \\
(y_{2} + 1)^{-1} & -y_{1}(y_{2} + 1)^{-2}
\end{bmatrix} \\
&= [y_{2}(y_{2} +1)^{-1}].[-y_{1}(y_{2}+1)^{-2}] - [y_{1}(y_{2} + 1)^{-2}].[(y_{2} + 1)^{-1}] \\
&= -y_{1}(y_{2} + 1)^{-2}
\end{align*}

Pelo teorema do Jacobiano, temos que:

\begin{equation*}
\int \ldots \int_{A}f(x_{1}, \ldots, x_{n})dx_{1} \ldots dx_{n} = \int \ldots \int_{g(A)}f(h_{1}(y_{1}, \ldots, y_{n}), \ldots , h_{n}(y_{1}, \ldots, y_{n}))|J(\underbar{x}, \underbar{y})|dy_{1} \ldots dy_{n}
\end{equation*}

Se $f$ é integrável em $A$, com $A \subseteq G_{0}$ e $h = g^{-1}$. Assim, usando os valores do exemplo \@ref(exm:motivacaoexponencial), temos que $X_{1} \sim exp(1), X_{2} \sim exp(1), X_{1} \perp X_{2}$, com densidade conjunta dada por $ f_{X_{1}X_{2}}(x_{1},x_{2}) = e^{-x_{1} + x_{2}}$, de modo que:

\begin{align*}
f(h_{1}(y_{1},y_{2}),h_{2}(y_{1},y_{2}))|J(\underbar{x},\underbar{y})| &= f\left(\frac{y_{1}y_{2}}{y_{2} + 1},\frac{y_{1}}{y_{2} + 1}\right)\big{|}-y_{1}(y_{2} + 1)^{-2}\big{|} \\
&= \exp\left(-\left[\frac{y_{1}y_{2}}{y_{2} + 1}+\frac{y_{1}}{y_{2} + 1}\right]\right)y_{1}(y_{2} + 1)^{-2} \\
&= e^{-y_{1}}y_{1}(y_{2} + 1)^{-2}
\end{align*}

Que é a mesma densidade conjunta encontrada para $Z$ e $W$ no exemplo \@ref(exm:motivacaoexponencial).

:::

### Notas

1. Sendo $f$ a densidade de $X_{1}, \ldots, X_{n}$ e $P((X_{1}, \ldots, X_{n}) \in G_{0}) = 1$, se $Y_{i} = g_{i}(x_{1}, \ldots, x_{n}); i = 1, \ldots, n$, e $\mathcal{B} \subseteq G$, com $\mathcal{B}$ boreliano. Então:

\begin{align*}
P((Y_{1},\ldots,Y_{n}) \in \mathcal{B}) &= P((X_{1}, \ldots, X_{n}) \in h(\mathcal{B})) \\
&= \int \dots \int_{h(\mathcal{B})}f(x_{1},\ldots,x_{n})dx_{1} \ldots dx_{n} \\
&= \int \dots \int_{\mathcal{B}}f(h_{1}(x_{1}, \ldots, x_{n}),\ldots,h_{n}(x_{1},\ldots,x_{n}))\big{|}J(\underbar{x},\underbar{y})\big{|}dy_{1} \ldots dy_{n}
\end{align*}

2. $P((Y_{1},\ldots,Y_{n}) \in G) = P((X_{1}, \ldots, X_{n}) \in h(G)) = P((X_{1}, \ldots, X_{n}) \in G_{0}) = 1$. De modo análogo:

\begin{align*}
P((Y_{1}, \ldots, Y_{n}) \in \mathcal{B}) &= P((Y_{1}, \ldots, Y_{n}) \in \mathcal{B} \cap G) \\
&= \int \dots \int_{\mathcal{B} \cap G}f(h(y))\big{|}J(\underbar{x},\underbar{y})\big{|}dy_{1} \ldots dy_{n}
\end{align*}

::: {.theorem}

Sob as condições impostas no início da seção, a densidade conjunta de $(Y_{1}, \ldots, Y_{n})$ é dada por:

\begin{equation*}
f_{Y_{1}\ldots Y_{n}} = \begin{cases}
f_{X}(h_{1}(y_{1},\ldots,y_{n}), \ldots, h_{n}(y_{1},\ldots,y_{n}))\big{|}J(\underbar{x},\underbar{y})\big{|} & ,y \in G \\
0 & ,c.c.
\end{cases}
\end{equation*}

:::

### Propriedades do Jacobiano

Podemos inverter a ordem das variáveis no Jacobiano, seguindo a seguinte propriedade:

\begin{equation}
J(\underbar{x},\underbar{y}) = \left(J(\underbar{y},\underbar{x})\right)^{-1}\big{|}_{\underbar{x} = h(y)}
(\#eq:inversajacobiano)
\end{equation}

::: {.example}

Retornando ao problema apresentado no exemplo \@ref(exm:motivacaoexponencial):

\begin{align*}
y_{1} = x_{1} + x_{2} \; \; &\; \; \; y_{2} = x_{1}x_{2}^{-1} \\
\frac{\partial y_{1}}{\partial x_{1}} = 1 \; \; &\; \; \; \frac{\partial y_{1}}{\partial x_{2}} = 1 \\
\frac{\partial y_{2}}{\partial x_{1}} = x_{2}^{-1} \; \; &\; \; \; \frac{\partial y_{2}}{\partial x_{2}} = -x_{1}(x_{2})^{-2}
\end{align*}

De modo que podemos agora encontrar o Jacobiano com relação aos valores das derivadas parciais dos $y$'s, e invertê-lo para encontrar o Jacobiano dos $x$'s:

\begin{align*}
J(\underbar{y},\underbar{x}) = \det \begin{bmatrix}
1 & 1 \\
x_{2}^{-1} & -x_{1}(x_{2})^{-2}
\end{bmatrix} &= (x_{2})^{-2}(x_{2} + x_{1})(-1) \\
&= \left(\frac{y_{2} + 1}{y_{1}}\right)^{2}\left(\frac{y_{1}}{y_{2} + 1} + \frac{y_{1}y_{2}}{y_{2} + 1}\right)(-1) \\
&= \frac{(y_{2} + 1)^{2}}{(y_{1})^{2}}\frac{y_{1}(y_{2} + 1)}{y_{2} + 1}(-1) \\
&= -\frac{(y_{2} + 1)^{2}}{y_{1}} = -y_{1}^{-1}(y_{2} + 1)^{2} = \frac{1}{J(\underbar{x},\underbar{y})}
\end{align*}

:::

Temos que, se $g:G_{0} \to G$, com $G_{0},G \subseteq \mathbb{R}^{n}$ abertos, se $g(x_{1}, \ldots, x_{n}) = (y_{1}, \ldots, y_{n})$, então $g$ é bijetiva e $h = g^{-1}$.

::: {.example}

Seja $X \sim U(0,1)$ e $Y = -ln(X)$. Temos que $G_{0} = (0,1)$, e $g(x) = -ln(x)$, de modo que $G = (0, \infty)$. Então:

\begin{align*}
g^{-1}(y) &= h(y) = \exp(-y) = e^{-y} \\
\frac{\partial}{\partial y}(g^{-1}(y)) &= -e^{-y} = J(x,y)
\end{align*}

Assim, para encontrar $P(Y \le y)$, teremos:

\begin{align*}
P(Y \le y) &= P(-\ln(X) \le y) \\
&= P(\ln(X) \ge -y) \\
&= P(X \ge e^{-y}) \\
&= 1 - P(X \le e^{-y}) \\
&= 1 - e^{-y} = F_{Y}(y) \Longrightarrow f_{Y}(y) = e^{-y}
\end{align*}

Pelo Jacobiano, teremos:

\begin{equation*}
f_{Y}(y) = f_{X}(h(y)).|J| = 1 . e^{-y}
\end{equation*}

:::

::: {.theorem}

Sejam $G_{1}, G_{2}, \ldots, G_{k}$ disjuntos tais que $P\left(\underbar{X} \in \bigcup_{i=1}^{k}G_{i}\right) = 1$, tal que $g\big{|}_{G_{l}}$ é 1:1 para todo $l = 1, \ldots, k$. Denotamos por $h^{(l)}$ a inversa de $g$ em $G_{l}$, e definimos assim o Jacobiano local $J_{l}(\underbar{x},\underbar{y})$ como:

\begin{equation*}
f_{Y}(\underbar{y}) = \begin{cases}
\sum_{l=1}^{k}f\left(h^{(l)}(y)\right)|J_{l}(\underbar{x},\underbar{y})| & ;\underbar{y} \in G_{l} \\
0 & c.c.
\end{cases}
\end{equation*}

:::

::: {.example}

Sejam $X \sim N(0,1)$ e $Y = X^{2}$. Sabemos que $y = x^{2}$ não é bijetiva, mas podemos considerar a seguinte partição em que essa função seja localmente bijetiva: $G_{1} = (-\infty, 0)$ e $G_{2} = (0, \infty)$. Então, em $G_{1}, h^{(1)}(y) = -\sqrt{y}$, e em $G_{2}, h^{(2)}(y) = \sqrt{y}$, de modo que os jacobianos locais serão:

\begin{align*}
J_{1}(x,y) &= \frac{\partial}{\partial y}h^{(1)}(y) = - \frac{1}{2\sqrt{y}} \\
J_{2}(x,y) &= \frac{\partial}{\partial y}h^{(2)}(y) = \frac{1}{2\sqrt{y}}
\end{align*}

Assim, a densidade de $Y$ será dada por:

\begin{align*}
f_{Y}(y) &= f_{X}\left(h^{(1)}(y)\right)\big{|}J_{1}(x,y)\big{|} + f_{X}\left(h^{(2)}(y)\right)\big{|}J_{2}(x,y)\big{|} \\
&= \frac{1}{\sqrt{2\pi}}\exp\left(-\frac{1}{2}y\right)\frac{1}{2\sqrt{y}} + \frac{1}{\sqrt{2\pi}}\exp\left(-\frac{1}{2}y\right)\frac{1}{2\sqrt{y}} \\
&= \begin{cases}
\frac{1}{\sqrt{2\pi}}y^{-\frac{1}{2}}e^{-\frac{1}{2}y} & ,y>0 \\
0 & ,c.c.
\end{cases}
\end{align*}

Ou seja, $Y \sim Gama\left(\frac{1}{2}, \frac{1}{2}\right)$, ou $Y \sim \chi^{2}(1)$.

:::

**Notas**:

- Se $X_{1}, \ldots, X_{n}$ são iid, com $X_{i} \sim N(0,1) \Rightarrow X_{1}^{2} + \ldots + X_{n}^{2} \sim \chi^{2}(n)$;
- Se $X \sim N(0,1), Y \sim \chi^{2}(n)$, com $X \perp Y \Rightarrow \frac{x}{\sqrt{y/n}} \sim t(n)$;
- Sejam $X_{1}, \ldots, X_{n}$, iid, com $X_{i} \sim N(0,1)$, com $\bar{x} = \frac{\sum_{i=1}^{n}x_{i}}{n}, s^{2} = \frac{1}{n-1}\sum_{i=1}^{n}(x_{i} - \bar{x})^{2}$:
  1. $\frac{\bar{x}\sqrt{n}}{\sigma} \sim N(0,1)$;
  2. $\frac{(n-1)s^{2}}{\sigma^{2}} \sim \chi^{2}(n-1)$;
  3. $\frac{\bar{x}\sqrt{n}}{s} \sim t(n-1)$;
  4. $\bar{x} \perp s^{2}$.
- Se $X \sim \chi^{2}(k), Y \sim \chi^{2}(n), X \perp Y \Rightarrow \frac{X/k}{Y/n} \sim F(k,n)$;
- Se $T \sim t(n) \Rightarrow T^{2} \sim F(1,n)$.

\newpage

## Exercícios

::: {.exercise name="BJ 2.1"}

Seja $X$ o número de caras obtidas em 4 lançamentos de uma moeda honesta. Desenhe o gráfico da função de distribuição de $X$.

:::

::: {.proof name="Resposta"}

Sabemos que $X \sim \mathrm{Bin}(4,0.5)$, então o gráfico da distribuição de $X$ será dada por:

```{r distacummoeda, echo=F}
x <- 0:4
Fx <- pbinom(x, 4, 0.5)
n <- length(x)
plot(x = NA, y = NA, pch = NA,
     xlim = c(0, max(x)),
     ylim = c(0, 1),
     xlab = "X",
     ylab = expression(F[X](x)),
     main = "Distribuição acumulada")
points(x = x[-n], y = Fx[-1], pch=19)
for(i in 1:(n-1)) points(x=x[i+0:1], y=Fx[c(i,i)+1], type="l")
points(x = x[-1], y = Fx[-1], pch=1)
```

:::

::: {.exercise name="BJ 2.2"}

Um ponto é selecionado, ao acaso, do quadrado unitário $[0,1] \times [0,1]$. Seja $X$ a primeira coordenada do ponto selecionado. Faça o gráfico da função de distribuição de $X$.

:::

::: {.proof name="Resposta"}

Como o ponto é escolhido ao acaso, cada uma das coordenadas seguirá uma distribuição uniforme entre 0 e 1, de modo que $X \sim \mathrm{U}(0,1)$, e o gráfico da distribuição será dado por:

```{r distacumx, echo=F}
curve(punif, -0.1, 1.1, xlab = "X", ylab = expression(F[X](x)), main = "Distribuição acumulada")
```

:::

::: {.exercise name="BJ 2.4"}

Seja $X$ uma variável aleatória com distribuição de Poisson, com parâmetro $\lambda > 0$. Mostre que a função de distribuição de $X$ é

\begin{equation*}
F(X) = \begin{cases}
\frac{1}{n!}\int_{\lambda}^{\infty}e^{-t}t^{n}dt & ,\text{se } n \le x < n+1, n = 0,1,2,\ldots \\
0 & ,\text{se } x <0
\end{cases}
\end{equation*}

:::

::: {.proof name="Resposta"}

Sabemos que a função densidade de probabilidade de $X$ é dada por $f(x) = \frac{e^{-\lambda}\lambda^{x}}{x!}$. Suponha que, para $x = k$ a função de distribuição anterior é verdadeira. Assim, mostremos por indução que a mesma é válida para $x = k+1$.

\begin{align*}
F(k+1) &= \frac{1}{(k+1)!}\int_{\lambda}^{\infty}e^{-t}t^{k+1}dt \Rightarrow \begin{bmatrix}
u = t^{k+1}dt & dv = e^{-t} \\
du = (k+1)t^{k} & v = -e^{-t}
\end{bmatrix}\\
&= \frac{1}{(k+1)!}\left[-e^{-t}t^{k+1}\Big{|}_{\lambda}^{\infty} + \int_{\lambda}^{\infty}e^{-t}(k+1)t^{k}dt\right] \\
&= \frac{1}{(k+1)!}\left[e^{-\lambda}\lambda^{k+1} + (k+1)\int_{\lambda}^{\infty}e^{-t}t^{k}dt\right] \\
&= \frac{e^{-\lambda}\lambda^{k+1}}{(k+1)!} + \frac{1}{k!}\int_{\lambda}^{\infty}e^{-t}t^{k}dt = P(X = k+1) + F(k)
\end{align*}

Assim, $F(k + 1) = F(k) + P(X = k+1)$, de modo que a função apresentada é sim a distribuição de $X$.

:::

::: {.exercise name="BJ 2.5"}

Suponha que a vida útil de um certo tipo de lâmpada tenha distribuição exponencial com parâmetro $\lambda$.

**a)** Seja $T$ o tempo de vida de uma lâmpada desse tipo. Mostre que:

\begin{equation*}
P(T > t+s | T>t) = P(T > s), \; \forall s,t>0
\end{equation*}

**b)** Suponha que $\lambda = 3$ quando a vida é expressa em dias. Uma lâmpada solitária é ligada em uma sala no instante $t = 0$. Um dia depois, você entra na sala e fica ali durante 8 horas, saindo no final desse período.

- *(i)* Qual a probabilidade de que você entre na sala quando já está escura?
- *(ii)* Qual a probabilidade de você entrar na sala com a lâmpada ainda acesa e sair da sala depois da lâmpada queimar?

:::

::: {.proof name="Resposta"}

**a)**

Temos que $[T > t+s] \subset [T > t] \Rightarrow P(T > t+s, T > t) = P(T > t+s)$. Além disso, como $T \sim \mathrm{Exp}(\lambda) \Rightarrow P(T > t) = 1 - P(T \le t) = 1 - F_{T}(t) = 1 - (1 - e^{-\lambda t}) = e^{-\lambda t}$. Desse modo:

\begin{align*}
P(T > t+s | T > t) = \frac{P(T > t+s,T > t)}{P(T > t)} &= \frac{P(T > t+s)}{P(T > t)} \\
&= \frac{e^{-\lambda(t+s)}}{e^{-\lambda t}} \\
&= e^{-\lambda s} = 1 - F_{T}(s) = P(T > s)
\end{align*}

**b)**

*(i)* Caso depois de um dia a lâmpada já esteja apagada, então $T \le 1$, de modo que:

\begin{equation*}
P(T \le 1) = 1 - e^{-3.1} = 1 - e^{-3}
\end{equation*}

*(ii)* Caso a lâmpada ainda esteja acesa depois de um dia, mas tenha queimado antes de 8 horas ($\frac{1}{3}$ de dia) dado que não tenha queimado no primeiro dia, queremos encontrar a probabilidade do seguinte evento: $P\left(T \le 1 + \frac{1}{3}|T > 1\right) = 1 - P\left(T > 1 + \frac{1}{3}|T > 1\right)$, que utilizando o resultado obtido em **(a)**, temos que será:

\begin{align*}
P\left(T \le 1 + \frac{1}{3}|T > 1\right) &= 1 - P\left(T > 1 + \frac{1}{3}|T > 1\right) \\
&= 1 - P\left(T > \frac{1}{3}\right) \\
&= 1 - e^{-3.\frac{1}{3}} = 1 - e^{-1}
\end{align*}

:::

::: {.exercise name="BJ 2.6"}

Seja $X$ uma variável aleatória com densidade:

\begin{equation*}
f(x) = \begin{cases}
cx^{2} & ,\text{se } -1 \le x \le 1 \\
0 & ,\text{c.c.}
\end{cases}
\end{equation*}

**a)** Determine o valor da constante $c$.

**b)** Ache o valor $\alpha$ tal que $F_{X}(\alpha) = \frac{1}{4}$.

:::

::: {.proof name="Resposta"}

**a)**

Temos que $\int_{-\infty}^{\infty}f(x)dx = 1 \Rightarrow \int_{-1}^{1}cx^{2}dx = 1$. Assim:

\begin{align*}
\int_{-1}^{1}cx^{2}dx &= c\int_{-1}^{1}x^{2}dx \\
&= c\left(\frac{x^{3}}{3}\Big{|}_{-1}^{1}\right) \\
&= c\left(\frac{1}{3} + \frac{1}{3}\right) = \frac{2c}{3} \Longrightarrow c = \frac{1}{\frac{2}{3}} = \frac{3}{2}
\end{align*}

**b)**

Podemos calcular a distribuição de $X$ como segue:

\begin{align*}
F_{X}(x) &= \frac{3}{2}\int_{-1}^{x}x^{2}dx \\
&= \frac{3}{2}\left(\frac{x^{3}}{3}\Big{|}_{-1}^{x}\right) \\
&= \frac{3}{2}\left(\frac{x^{3} + 1}{3}\right) = \frac{x^{3} + 1}{2}
\end{align*}

Assim, podemos encontrar o valor $\alpha$ tal que $F_{X}(\alpha) = \frac{1}{4}$:

\begin{align*}
\frac{x^{3} + 1}{2} &= \frac{1}{4} \\
x^{3} + 1 &= \frac{1}{2} \\
x &= \sqrt[3]{-\frac{1}{2}}
\end{align*}

:::

::: {.exercise name="BJ 2.7"}

Uma variável aleatória $X$ tem função de distribuição:

\begin{equation*}
F(X) = \begin{cases}
0 & ,\text{se }x<0 \\
x^{3} & ,\text{se }0 \le x \le 1 \\
1 & ,\text{se }x>1
\end{cases}
\end{equation*}

Qual a densidade de $X$?

:::

::: {.proof name="Resposta"}

Podemos encontrar a função densidade de probabilidade de $X$ a partir da derivação em partes da distribuição acumulada:

\begin{equation*}
f_{X}(x) = \frac{\partial}{\partial x}F_{X}(x) = \frac{\partial}{\partial x}(x^{3}) = 3x^{2}
\end{equation*}

De modo que a densidade será:

\begin{equation*}
f_{X}(x) = \begin{cases}
0 & ,\text{se }x<0 \\
3x^{2} & ,\text{se }0 \le x \le 1 \\
0 & ,\text{se }x>1
\end{cases}
\end{equation*}

:::

::: {.exercise name="BJ 2.9"}

Seja $X$ uma variável aleatória com densidade:

\begin{equation*}
f(x) = \begin{cases}
\frac{1}{(1+x)^{2}} &, \text{se } x > 0 \\
0 & ,\text{c.c.}
\end{cases}
\end{equation*}

Seja $Y = \max(X,c)$, onde $c$ é uma constante maior que 0.

**a)** Ache a função de distribuição de $Y$.

**b)** Decomponha $F_{Y}$ em partes discreta, absolutamente contínua e singular.

:::

::: {.proof name="Resposta"}

**a)**

Como $Y = \max(X,c)$, separaremos em dois casos:

$\mathbf{\min(X,c) = X} \Rightarrow \mathbf{X \le c}$:

\begin{align*}
F_{Y}(y) &= \int_{0}^{c}\frac{1}{(1+y)^{2}}dy \Rightarrow \begin{bmatrix}
u = (1+y) & du = dy \\
a = 1 & b = c+1
\end{bmatrix}\\
&= \int_{1}^{c+1}u^{-2}du \\
&= -u^{-1}\Big{|}_{1}^{c+1} = -(c+1)^{-1} - (-1^{-1}) = 1 - \frac{1}{c+1} = \frac{c}{c+1}
\end{align*}

Como $c$ é uma constante, esse valor será a probabilidade pontual $P(\max(X,c) = c) = P(Y = c) = \frac{c}{c+1}$.

$\mathbf{\min(X,c) = c} \Rightarrow \mathbf{X > c}$:

\begin{align*}
F_{Y}(y) = P(Y = c) + P(c < Y \le y) &= \frac{c}{c+1} + \int_{c}^{y}\frac{1}{(1+y)^{2}}dy \Rightarrow \begin{bmatrix}
u = (1+y) & du = dy \\
a = c+1 & b = y+1
\end{bmatrix}\\
&= \frac{c}{c+1} + \left(\int_{c+1}^{y+1}u^{-2}du\right) \\
&= \frac{c}{c+1} + \left(-u^{-1}\Big{|}_{c+1}^{y+1}\right) \\
&= \frac{c}{c+1} + \frac{1}{c+1} - \frac{1}{y+1} \\
&= \frac{c + 1}{c + 1} - \frac{1}{y+1} = 1 - \frac{1}{y+1} = \frac{y}{y+1}
\end{align*}

Assim, a distribuição de $Y$ será:

\begin{equation*}
F_{Y}(x) = \begin{cases}
0 &,\text{se }x \le 0 \\
\frac{c}{c+1} &,\text{se } 0 < x \le c \\
\frac{x}{x+1} &,\text{se } x > c
\end{cases} \Rightarrow F_{Y}(y) = \begin{cases}
0 &,\text{se } y < c \\
\frac{y}{y+1} &,\text{se } y \ge c
\end{cases}
\end{equation*}

**b)**

A parte discreta envolve o salto que ocorre em $Y = c$, de tamanho $\frac{c}{c+1}$. Os demais pontos são absolutamente contínuos. Assim, não temos partes singulares.

:::

::: {.exercise name="BJ 2.10"}

Se $X$ é uma variável aleatória com distribuição exponencial de parâmetro $\lambda > 0$, qual a distribuição da variável aleatória $Y = \min(\lambda, X)$

:::

::: {.proof name="Resposta"}

De maneira similar ao caso anterior, como $Y = \min(\lambda,X)$, separaremos $Y$ em dois casos:

$\mathbf{\min(\lambda,X) = X} \Rightarrow \mathbf{X \le \lambda}$:

\begin{align*}
F_{Y}(y) &= \int_{0}^{y}\lambda e^{-\lambda y}dy \\
&= \frac{\lambda}{\lambda}\left(-e^{-u}\Big{|}_{0}^{\lambda y}\right) = 1 - e^{-\lambda y}
\end{align*}

$\mathbf{\min(\lambda,X) = \lambda} \Rightarrow \mathbf{\lambda < X < \infty}$:

\begin{align*}
F_{Y}(y) = P(Y \le \lambda) + P(Y > \lambda) &= 1 - e^{-\lambda \lambda} + \int_{\lambda}^{\infty}\lambda e^{-\lambda y}dy \\
&=  1 - e^{-\lambda^{2}} + \frac{\lambda}{\lambda}\left(-e^{-u}\Big{|}_{\lambda^{2}}^{\infty}\right) = 1 - e^{-\lambda^{2}} + e^{-\lambda^{2}} = 1
\end{align*}

De modo que a distribuição de $Y$ é dada por:

\begin{equation*}
F_{Y}(y) = \begin{cases}
0 &,\text{se } x \le 0 \\
1 - e^{-\lambda y} &,\text{se } 0 < x \le \lambda \\
1 &,\text{se } x > \lambda
\end{cases}
\end{equation*}

:::

::: {.exercise name="BJ 2.12"}

Determine a densidade de $Y = (b-a)X + a$, onde $X \sim U[0,1]$. Faça o gráfico da função de distribuição de $Y$.

:::

::: {.proof name="Resposta"}

Sabemos que $f_{X}(x) = I_{[0,1]}$ e pela proposição \@ref(prp:transfsoma) temos que quando $Y = bX + c$ então:

\begin{equation*}
f_{Y}(y) = \frac{1}{b}f_{X}\left(\frac{y - c}{b}\right)
\end{equation*}

Dessa forma, considerando que $b = (b-a)$ e $c = (a)$, então:

\begin{align*}
f_{Y}(y) &= \frac{1}{(b-a)}f_{X}\left(\frac{y-a}{b-a}\right) = \frac{1}{(b-a)}I_{[0,1]}
\end{align*}

De modo que $Y \sim U(a,b)$.

:::

::: {.exercise name="BJ 2.13"}

Se $X$ tem densidade $f(x) = e^{-2|x|}, -\infty < x < \infty$, qual a densidade de $Y = |X|$?

:::

::: {.proof name="Resposta"}

Como $F_{Y}(y) = P(Y \le y)$, temos que:

\begin{align*}
F_{Y}(y) &= P(Y \le y) \\
&= P(|X| \le y) \\
&= \int_{-y}^{y}e^{-2|x|}dx
\end{align*}

Como $f_{X}(x)$ é simétrica em torno de zero, temos que:

\begin{align*}
F_{Y}(y) = 2\int_{0}^{y}e^{-2|x|}dx &= 2\int_{0}^{y}e^{-2x}dx \\
&= \frac{2}{2}\int_{0}^{2y}e^{-u}du \\
&= -e^{-u}\Big{|}_{0}^{2y} \\
&= 1 - e^{-2y}
\end{align*}

Ou seja, $f_{Y}(y) = \frac{\partial}{\partial y}F_{Y}(y) = 2e^{-2y} \Rightarrow Y \sim \mathrm{Exp}(2)$.

:::

::: {.exercise name="BJ 2.14"}

Cinco pontos são escolhidos, independentemente e ao acaso, do intervalo $[0,1]$. Seja $X$ o número de pontos que pertencem ao intervalo $[0,c]$ onde $0 < c < 1$. Qual a distribuição de $X$?

:::

::: {.proof name="Resposta"}

Consideremos inicialmente o caso em que um ponto é escolhido ao acaso do intervalo $[0,1]$. Desse modo, por ser uniformemente distribuído no intervalo, a probabilidade de que o ponto pertença ao intervalo $[0,c]$ é o comprimento desse intervalo, de modo que:

\begin{equation*}
P(X = 0) = (1-c),P(X = 1) = c
\end{equation*}

Para dois pontos, temos que levar em consideração o caso em que $X = 1$, pois podem ocorrer duas formas diferentes de isso ocorrer: o primeiro ponto pertence ao intervalo e o segundo não, ou o segundo ponto pertence ao intervalo e o segundo não, de modo que:

\begin{equation*}
P(X = 0) = (1-c)^{2},P(X = 1) = 2c(1-c), P(X = 2) = c^{2}
\end{equation*}

É fácil perceber o padrão, de modo que $X \sim \mathrm{Bin}(5,c)$.

:::

::: {.exercise name="BJ 2.15"}

Determine a distribuição do tempo de espera até o segundo sucesso em uma sequência de ensaios de Bernoulli com probabilidade $p$ de sucesso.

:::

::: {.proof name="Resposta"}

Considerando que, nesse caso, o tempo de espera é discreto, vamos levar em consideração que foram necessários $n$ ensaios até o segundo sucesso, que ocorreu com probabilidade $p$. Assim, ocorreu algum sucesso entre os $(n-1)$ ensaios anteriores, também com probabilidade $p$, enquanto que os $(n-2)$ ensaios restantes foram fracassos, cada um com probabilidade $(1 - p)$.

Seja $X$ o número de ensaios necessários até o segundo sucesso. Como os ensaios são independentes, a probabilidade de que $X = 2$ será:

\begin{equation*}
P(X = 2) = (n-1)p^{2}(1-p)^{n-2}
\end{equation*}

Que podemos identificar como sendo proveniente de uma distribuição binomial negativa, que de modo geral descreve a probabilidade de serem necessários $X$ ensaios de Bernoulli independentes com probabilidade de sucesso $p$ até se obter o $r$-ésimo sucesso, com a seguinte densidade:

\begin{equation*}
X \sim \mathrm{NegBin}(r,p) \Rightarrow f_{X}(x) = \binom{x-1}{r-1}p^{r}(1-p)^{x-r}
\end{equation*}

:::

::: {.exercise name="BJ 2.17"}

**a)**

Demonstre que a função

\begin{equation*}
F(x,y) = \begin{cases}
1 - e^{-x-y} &,\text{se }x \ge 0, y \ge 0 \\
0 &,\text{c.c.}
\end{cases}
\end{equation*}

não é função de distribuição de um vetor aleatório.

**b)**

Mostre que a seguinte função é função de distribuição de algum $(X,Y)$:

\begin{equation*}
F(x,y) = \begin{cases}
(1 - e^{-x})(1 - e^{-y}) &,\text{se }x \ge 0, y \ge 0 \\
0 &,\text{c.c.}
\end{cases}
\end{equation*}

:::

::: {.proof name="Resposta"}

**a)**

Para que $F$ seja uma função de distribuição de probabilidade, é necessário que ela siga as propriedades enunciadas no início do capítulo, a ver:

- $F_{1}$: é não-decrescente em cada uma das coordenadas:

Seja $x_{1} \le x_{2}$. Teremos que:

\begin{align*}
F(x_{1},y) &\stackrel{?}{\le} F(x_{2},y) \\
1 - e^{-y}e^{-x_{1}} &\stackrel{?}{\le} 1 - e^{-y}e^{-x_{2}} \\
-e^{-y}e^{-x_{1}} &\stackrel{?}{\le} -e^{-y}e^{-x_{2}} \\
e^{-x_{1}} &\stackrel{?}{\ge} e^{-x_{2}}
\end{align*}

Como $x_{1} \le x_{2} \Rightarrow e^{-x_{1}} \ge e^{-x_{2}}$, então a função é não decrescente em $X$. Para $Y$ se obtém de maneira análoga.

- $F_{2}$: é contínua à direita:

Como a função $F$ é absolutamente contínua no espaço amostral de $X$ e $Y$, ela será também contínua à direita.

- $F_{3}$: limites do espaço amostral:

\begin{equation*}
\lim_{x \to 0}F(x,y) = 0 \;\;\; \lim_{y \to 0}F(x,y) = 0 \;\;\; \lim_{x \to \infty,y \to \infty}F(x,y) = 1
\end{equation*}

- $F_{4}: \Delta_{1,I_{1}}\Delta_{2,I_{2}}\ldots\Delta_{n,I_{n}}(F(x_{1},\ldots,x_{n})) \ge 0 \; \forall I_{k} = (a_{k}, b_{k}]; a_{k} \le b_{k}, k=1,\ldots,n$.

\begin{align*}
P(a < X \le b, c < Y \le d) &= F(b,d) - F(a,d) - F(b,c) + F(a,c) \ge 0 \\
&= 1 - e^{-b-d} - 1 + e^{-a-d} - 1 + e^{-b-c} + 1 - e^{-a-c} \ge 0 \\
&= e^{-a-d} + e^{-b-c} - e^{-b-d} - e^{-a-c} \ge 0 \\
e^{-a-d} + e^{-b-c} &\ge e^{-b-d} + e^{-a-c}
\end{align*}

E como $a < b$ e $c < d$, temos que $e^{-a-d} < e^{-b-d}$ e $e^{-b-c} < e^{-a-c}$, de modo que a última desigualdade é falsa. Assim, a avaliação dessa probabilidade será negativa, para qualquer conjunto de pontos $(a,b] \in X, (c,d] \in Y$, o que mostra que $F$ não é uma função distribuição de probabilidade.

**b)**

Semelhante ao caso anterior, vejamos se $F$ segue as propriedades $F_{1}$ a $F_{4}$:

- $F_{1}$: é não-decrescente em cada uma das coordenadas:

Como podemos separar essa distribuição em um produto de duas partes, uma que depende apenas de $x$ e outra que
depende apenas de $y$, podemos analisar cada caso separadamente. Como $(1 - e^{-x})$ é não-decrescente em $[0, \infty)$, ela será não-decrescente em $y$ também.

- $F_{2}$: é contínua à direita:

Como a função $F$ é absolutamente contínua no espaço amostral de $X$ e $Y$, ela será também contínua à direita.

- $F_{3}$: limites do espaço amostral:

\begin{equation*}
\lim_{x \to 0}F(x,y) = 0 \;\;\; \lim_{y \to 0}F(x,y) = 0 \;\;\; \lim_{x \to \infty,y \to \infty}F(x,y) = 1
\end{equation*}

- $F_{4}: \Delta_{1,I_{1}}\Delta_{2,I_{2}}\ldots\Delta_{n,I_{n}}(F(x_{1},\ldots,x_{n})) \ge 0 \; \forall I_{k} = (a_{k}, b_{k}]; a_{k} \le b_{k}, k=1,\ldots,n$.

\begin{align*}
P(a < X \le b, c < Y \le d) &= F(b,d) - F(a,d) - F(b,c) + F(a,c) \ge 0 \\
&= (1 - e^{-b})(1 - e^{-d}) - (1 - e^{-a})(1 - e^{-d}) - (1 - e^{-b})(1 - e^{-c}) + (1 - e^{a})(1 - e^{c}) \ge 0 \\
&= (1 - e^{-b})(1 - e^{-d} - 1 + e^{-c}) - (1 - e^{-a})(1 - e^{-d} - 1 + e^{-c}) \ge 0 \\
&= (1 - e^{-b})(e^{-c} - e^{-d}) - (1 - e^{-a})(e^{-c} - e^{-d}) \ge 0 \\
(1 - e^{-b})(e^{-c} - e^{-d}) &\ge (1 - e^{-a})(e^{-c} - e^{-d}) \\
\end{align*}

E como $a < b$, temos que $(1 - e^{-a}) < (1 - e^{-b})$, de modo que a última desigualdade é verdadeira. Assim, a avaliação dessa probabilidade será positiva, para qualquer conjunto de pontos $(a,b] \in X, (c,d] \in Y$, o que mostra que $F$ é uma função distribuição de probabilidade.

:::

::: {.exercise name="BJ 2.18"}

Uma urna contém três bolas numeradas 1, 2 e 3. Duas bolas são retiradas sucessivamente da urna, ao acaso e sem reposição. Seja $X$ o número da primeira bola tirada e $Y$ o número da segunda.

**a)** Descreva a distribuição conjunta de $X$ e $Y$.

**b)** Calcule $P(X < Y)$.

:::

::: {.proof name="Resposta"}

**a)**

Temos que a tabela da distribuição conjunta de $X$ e $Y$ será:

\begin{center}
\begin{tabular}{|l||*{3}{c|}}\hline
\backslashbox{$X$}{$Y$}
&\makebox[1em]{1}&\makebox[1em]{2}&\makebox[1em]{3}\\\hline\hline
1 & 0 & 1/6 & 1/6 \\ \hline
2 & 1/6 & 0 & 1/6 \\ \hline
3 & 1/6 & 1/6 & 0 \\ \hline
\end{tabular}
\end{center}

Assim, podemos ver que a distribuição conjunta será:

\begin{equation*}
f_{XY}(i,j) = P(X = i, Y = j) = \begin{cases}
0 & ,\text{se }i = j, \; i,j = 1,2,3 \\
\frac{1}{6} & ,\text{se }i \neq j
\end{cases}
\end{equation*}

**b)**

Temos que $P(X < Y)$ será dado por:

\begin{equation*}
P(X < Y) = P(X = 1, Y = 2) + P(X = 1, Y = 3) + P(X = 2, Y = 3) = \frac{1}{6} + \frac{1}{6} + \frac{1}{6} = \frac{1}{2}
\end{equation*}

:::

::: {.exercise name="BJ 2.19"}

Dizemos que a distribuição conjunta de $X_{1},\ldots,X_{n}$ é invariante para permutações se toda permutação das $X_{i}$ tem a mesma distribuição, i.e., se $(X_{\pi_{1}},\ldots,X_{\pi_{n}}) \sim (X_{1},\ldots,X_{n})$ para toda permutação $(\pi_{1},\ldots,\pi_{n})$ do vetor $(1,\ldots,n)$.

**a)** Mostre que se $(X,Y) \sim (Y,X)$ e $X$ e $Y$ possuem densidade conjunta $f(x,y)$, então $P(X < Y) = P(X > Y) = \frac{1}{2}$, com $P(X = Y) = 0$.

**b)** Generalize o item **(a)**, provando que se a distribuição conjunta de $X_{1},\ldots,X_{n}$ é invariante para permutações e $X_{1},\ldots,X_{n}$ possuem densidade conjunta $f(x_{1},\ldots,x_{n})$, então:

\begin{equation*}
P(X_{1} < X_{2} < \ldots < X_{n}) = P(X_{\pi_{1}} < X_{\pi_{2}} < \ldots < X_{\pi_{n}}) = \frac{1}{n!}
\end{equation*}

e $P(X_{i} = X_{j} \text{ para algum par }(i,j) \text{ tal que } i \neq j) = 0$

:::

\newpage
