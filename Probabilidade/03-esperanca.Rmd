---
title: "Notas de Aula - Capítulo 3"
subtitle: "Probabilidade"
author: "Caio Gomes Alves"
date: "`r format(Sys.Date(), '%d/%m/%Y')`"
header-includes:
  - \usepackage{amsmath}
  - \usepackage{tikz}
  - \usepackage{pgfplots}
  - \usepackage{caption}
  - \usepackage{subcaption}
  - \usepackage{cancel}
  - \usepackage{mathtools}
  - \usepackage{annotate-equations}
output:
  bookdown::pdf_document2:
    toc: false
---

# Esperança

## Definição

::: {.definition}

Se $X$ é uma variável aleatória com distribuição $F$, a esperança de $X$ é definida por $E(X = \int_{-\infty}^{\infty}x dF(x)$, sempre que a integral estiver bem definida.

:::

**Convenção**: Se $E(X) < \infty$, então $X$ é integrável.

**Nota**: $\int_{-\infty}^{\infty}xdF(x)$ é bem definida se $\int_{0}^{\infty}xdF(x)$ ou $\int_{-\infty}^{0}xdF(x)$ for finita, já que $\int_{-\infty}^{\infty}xdF(x) = \underbrace{\int_{-\infty}^{0}xdF(x)}_{\mathclap{\mathbf{I} \le 0}} + \underbrace{\int_{0}^{\infty}xdF(x)}_{\mathclap{\mathbf{II} \ge 0}}$. Assim, podemos separar em quatro casos:

1. Se **I** e **II** são finitos, então $X$ é integrável;
2. Se **I** é finito e $\mathbf{II} = +\infty$, então $E(X) = +\infty$;
3. Se **II** é finito e $\mathbf{I} = -\infty$, então $E(X) = -\infty$;
4. Se $\mathbf{I} = -\infty$ e $\mathbf{II} = +\infty$, então $E(X)$ é indefinida.

**Propriedade**: $E(|X|) = \int |x| dF(x)$. Logo, $X$ é integrável se e somente se $E(|X|) < \infty$.

::: {.example}

$X \sim U(0,1),\; Y = \min\left(X,\frac{1}{2}\right)$:

\begin{align*}
P\left(Y = \frac{1}{2}\right) &= P\left(X > \frac{1}{2}\right) = 1 - F_{X}\left(\frac{1}{2}\right) = 1 - \frac{1}{2} = \frac{1}{2} = P_{Y}\left(Y = \frac{1}{2}\right)\\
E(Y) = \int_{-\infty}^{\infty}ydF(y) &= \int_{0}^{1/2}y.1dy + \frac{1}{2}P_{Y}\left(Y=\frac{1}{2}\right) \\
&= \frac{y^{2}}{2}\bigg{|}_{0}^{1/2} + \frac{1}{4} \\
&= \frac{1}{8} + \frac{1}{2} = \frac{3}{8}
\end{align*}

:::

::: {.proposition #separespe}

$E(X) = \int_{0}^{\infty}(1 - F(x))dx - \int_{-\infty}^{0}F(x)dx$. Disso, temos que:

- **a)** $\int_{0}^{\infty}xdF(x) = \int_{0}^{\infty}(1 - F(x))dx$;
- **b)** $\int_{-\infty}^{0}xdF(x) = - \int_{-\infty}^{0}F(x)dx$;

:::


::: {.proof name="Prova"}

Vejamos **(a)**: considere que $d(xF(x)) = F(x)dx + xd(F(x)) \Rightarrow xd(F(x)) = d(xF(x)) - F(x)dx$. Seja um $b > 0$:

\begin{align*}
\int_{0}^{b}xdF(x) &= \int_{0}^{b}d(xF(x)) - \int_{0}^{b}F(x)dx \\
&= xF(x)\bigg{|}_{0}^{b} - \int_{0}^{b}F(x)dx \\
&= bF(b) - \int_{0}^{b}F(x)dx \\
&= \int_{0}^{b}[F(b) - F(x)]dx
\end{align*}

Note que $\int_{0}^{b}xdF(x) \le \int_{0}^{\infty}[1 - F(x)]dx, \; \forall b>0$. Basta notar que $F(b) - F(x) \le 1 - F(x)$ e que $\int_{0}^{b}[1 - F(x)]dx \le \int_{0}^{\infty}[1 - F(x)]dx$. Logo:

\begin{equation*}
\int_{0}^{\infty}xdF(x) = \lim_{b \to \infty}\int_{0}^{b}xdF(x) \le \int_{0}^{\infty}[1 - F(x)]dx \Rightarrow \int_{0}^{\infty}xdF(x) \le \int_{0}^{\infty}[1 - F(x)]dx
\end{equation*}

Considere $\lambda > 0$ e $b > 0$, tais que:

\begin{align*}
\int_{0}^{b}[F(b) - F(x)]dx \ge \int_{0}^{\lambda}[1 - F(x)]dx &= \int_{0}^{\lambda}[F(b) - 1]dx + \int_{0}^{\lambda}[1 - F(x)]dx \\
&= \lambda[F(b) - 1] + \int_{0}^{\lambda}[1 - F(x)]dx \\
\int_{0}^{b}[F(b) - F(x)]dx &\ge \lambda[F(b) - 1] + \int_{0}^{\lambda}[1 - F(x)]dx
\end{align*}

Logo, como $\int_{0}^{\infty}xdF(x) = \lim_{b \to \infty}\int_{0}^{b}[F(b) - F(x)]dx \ge \lim_{b \to \infty}\{\lambda[F(b) - 1] + \int_{0}^{\lambda}[1 - F(x)]dx\} = \int_{0}^{\lambda}[1 - F(x)]dx$. Assim:

\begin{equation*}
\int_{0}^{\infty}xdF(x) \ge \lim_{\lambda \to \infty}\int_{0}^{\lambda}[1 - F(x)]dx = \int_{0}^{\infty}[1 - F(x)]dx
\end{equation*}

E como $\int_{0}^{\infty}xdF(x) \le \int_{0}^{\infty}[1 - F(x)]dx$, temos que $\int_{0}^{\infty}xdF(x) = \int_{0}^{\infty}[1 - F(x)]dx$

:::

::: {.corollary #esppositiva}

Se $X$ é tal que $X(\omega) \ge 0 \; \forall \omega \in \mathbb{R} \Rightarrow E(X) = \int_{0}^{\infty}[1 - F(x)]dx = \int_{0}^{\infty}P(X \ge x)dx$.

:::

::: {.example}

Seja $X \sim Exp(\lambda)$, qual a $E(X)$? Como o suporte de $X$ é $(0,\infty)$, aplica-se o corolário anterior, de modo que:

\begin{align*}
F_{X}(x) &= 1 - e^{-\lambda x} \Leftrightarrow P(X > x) = e^{-\lambda x} \\
E(X) &= \int_{0}^{\infty}e^{-\lambda x}dx = -\frac{1}{\lambda}e^{-\lambda x}\bigg{|}_{0}^{\infty} = \frac{1}{\lambda}
\end{align*}

:::

**Nota**: Suponha $X$ discreta e $X(\omega) \ge 0 \; \forall \omega$. Então:

\begin{align*}
E(X) = \sum_{n=0}^{\infty}P(X > n) &= \sum_{n=0}^{\infty}P(X \ge n+1) \\
&= \sum_{n=1}^{\infty}P(X \ge n)
\end{align*}

::: {.example}

Considere o lançamento de uma moeda até a 1ª cara. Suponha $p=$ probabilidade de cara e $(1 - p) =$ probabilidade de coroa, e $X =$ número de lançamentos até a primeira cara. Tome o evento $[X \ge n]$, logo:

\begin{equation*}
E(X) = \sum_{n=1}^{\infty}(1 - p)^{n-1} = \sum_{n=0}^{\infty}(1 - p)^{n} = \frac{1}{p}
\end{equation*}

:::

**Nota**: Sendo $X$ uma variável aleatória, temos pelo corolário \@ref(cor:esppositiva) que:

\begin{align*}
E(|X|) &= \int_{0}^{\infty}P(|X| > x)dx \\
&= \int_{0}^{\infty}\big[P(X > x) + P(X < -x)\big]dx \\
&= \int_{0}^{\infty}P(X > x)dx + \int_{0}^{\infty}P(X < -x)dx \\
&= \int_{0}^{\infty}(1-F(x))dx + \int_{0}^{\infty}F((-x)^{-})dx
\end{align*}

Onde $F((-x)^{-}) = \lim_{u \uparrow -x}F(u)$, que caso $F$ seja contínua, coincide com $F(-x)$. Logo:

\begin{equation*}
E(|X|) = \int_{0}^{\infty}(1 - F(x))dx + \int_{0}^{\infty}F(-x)dx
\end{equation*}

Já que $F$ pode ser descontínua em uma coleção enumerável de pontos. Agora, tomando a transformação de variável $y = -x \Leftrightarrow dy = -dx$:

\begin{align*}
E(|X|) &= \int_{0}^{\infty}(1 - F(x))dx + \int_{-\infty}^{0}F(y)dy \\
&= \int_{0}^{\infty}(1 - F(x))dx + \int_{-\infty}^{0}F(x)dx
\end{align*}

Utilizando os resultados **a** e **b** da proposição \@ref(prp:separespe), temos que:

\begin{align*}
E(|X|) &= \int_{0}^{\infty}xdF(x) - \int_{-\infty}^{0}xdF(x) \\
&= \int_{0}^{\infty}|x|dF(x) + \int_{-\infty}^{0}|x|dF(x) \\
&= \int_{-\infty}^{\infty}|x|dF(x)
\end{align*}

Onde $F$ é a acumulada de $X$, ao invés de $|X|$. Assim, a integrabilidade de $X$ depende da finitude de $\int_{0}^{\infty}xdF(x)$ e $\int_{-\infty}^{0}xdF(x)$, logo $X$ é integrável se $E(|X|) < \infty$.

## Propriedades da esperança

- $\mathbf{E_{1}}$: Se $X = c$, com $c$ uma constante, $E(X) = c$;
- $\mathbf{E_{2}}$**(monotonia)**: Se $X$ e $Y$ são variáveis aleatórias, com $X \le Y \Rightarrow E(X) \le E(Y)$, caso ambas as esperanças estejam bem definidas;

::: {.proof name="Prova"}

Seja $z$ um valor fixo. Se $Y \le z \Rightarrow X \le z$, logo $[Y \le z] \subseteq [X \le z]$, assim:

\begin{align*}
P(Y \le z) &\le P(X \le z) \\
F_{Y}(z) &\le F_{X}(z) \Longleftrightarrow 1 - F_{Y}(z) \ge 1 - F_{X}(z)
\end{align*}

E pela proposição \@ref(prp:separespe), temos que:

\begin{align*}
E(Y) = \int_{0}^{\infty}\big{[}1 - F_{Y}(z)\big{]}dz - \int_{-\infty}^{0}F_{Y}(z)dz &\ge \int_{0}^{\infty}\big{[}1 - F_{X}(z)\big{]}dz - \int_{-\infty}^{0}F_{X}(z)dz = E(X) \\
E(Y) &\ge E(X)
\end{align*}

:::

- $\mathbf{E_{3}}$**(linearidade)**:
  - **(i)** Se $E(X)$ é bem definida, $a,b \in \mathbb{R}$, então $E(aX + b) = aE(X) + b$;
  - **(ii)** $E(aX + bY) = aE(X) + bE(Y)$, caso o termo $aE(X) + bE(Y)$ esteja bem definido;
  - Note que se $E(X) = \infty \Rightarrow E(X - X) \neq E(X) - E(X)$.

::: {.proof name="Prova"}

Quando $a = 0; E(aX + b) = E(b) = b = 0E(X) + b$.

Quando $a > 0, b > 0; F_{aX + b}(x) = P(aX+b \le x) = P\left(X \le \frac{x-b}{a}\right) = F_{X}\left(\frac{x-b}{a}\right)$. Logo:

\begin{align*}
E(aX + b) &= \int_{0}^{\infty}\big{[}1 - F_{aX + b}(x)\big{]}dx - \int_{-\infty}^{0}F_{aX + b}(x)dx \\
&= \int_{0}^{\infty}\left[1 - F_{X}\left(\frac{x-b}{a}\right)\right]dx - \int_{-\infty}^{0}F_{X}\left(\frac{x-b}{a}\right)dx
\end{align*}

Tome $y = \frac{x-b}{a} \Rightarrow dy = \frac{1}{a}dx$. Então:

\begin{align*}
E(aX+b) &= \int_{-b/a}^{\infty}a\big{[}1 - F_{X}(y)\big{]}dy - \int_{-\infty}^{-b/a}aF_{X}(y)dy \\
&= a\left\{\int_{-b/a}^{\infty}\big{[}1 - F_{X}(y)\big{]}dy - \int_{-\infty}^{-b/a}F_{X}(y)dy\right\} \\
&= a\int_{0}^{\infty}\big{[}1 - F_{X}(y)\big{]}dy - a\int_{-\infty}^{0}F_{X}(y)dy + a\int_{-b/a}^{0}\big{[}1 - F_{X}(y)\big{]}dy + a\int_{-b/a}^{0}F_{X}(y)dy \\
&= aE(X) + a \int_{-b/a}^{0}dy \\
&= aE(X) + a \frac{b}{a} \\
&= aE(X) + b
\end{align*}

:::

- $\mathbf{E_{4}}$**(Desigualdade de Jansen)**: Seja $\varphi$ uma função convexa, definida na reta, com $X$ integrável, então:

\begin{equation}
E(\varphi(X)) \ge \varphi(E(X))
(\#eq:desigjansen)
\end{equation}

**Nota**: Caso $\varphi$ seja côncava:

\begin{equation*}
E(\varphi(X)) \le \varphi(E(X))
\end{equation*}

::: {.proof name="Prova para convexa"}

Tome $x_{0}$ e $\varphi(x_{0})$. Então existe uma reta $L$ tal que $L$ passe por $\varphi(x_{0})$ e $\varphi$ fica por cima de $L$. Logo temos a seguinte equação da reta:

\begin{equation*}
L(x) = \varphi(x_{0}) + \lambda(x - x_{0})
\end{equation*}

Onde $\lambda$ é alguma constante apropriada. Então para todo $x$ temos:

\begin{align*}
\varphi(x) &\ge L(x) = \varphi(x_{0}) + \lambda(x - x_{0}) \\
&\big{\Downarrow} \;\mathbf{E_{2}} \\
E(\varphi(x)) &\ge E(L(x)) \stackrel{\mathbf{E_{1},E_{3}}}{=} \varphi(x_{0}) + \lambda\left[E(x) - x_{0}\right]
\end{align*}

Que vale para $x_{0} = E(x)$, de modo que $E(\varphi(x)) \ge \varphi(E(x)) + \lambda\left[E(x) - E(x)\right]$, então:

\begin{equation*}
E(\varphi(x)) \ge \varphi(E(x))
\end{equation*}

A prova para funções côncavas segue a mesma metodologia, com a inversão da desigualdade.

:::

### Critério de integrabilidade

Suponha que $X$ é uma variável aleatória dominada por $Y$ (ou seja, $X \le Y$), sendo $Y$ uma variável aleatória integrável. $X$ é integrável? Temos que:

\begin{equation*}
X \le Y \Rightarrow E(X) \le E(Y)
\end{equation*}

Se $X$ e $Y$ são tais que $Y \ge 0$ e $Y$ é integrável e $|X| \le Y \Rightarrow 0 \le |X| \le Y$, e como consequência:

\begin{equation*}
0 \le E(X) \le E(Y) < \infty \Longrightarrow X \text{ é integrável}
\end{equation*}

De maneira similar, seja $X$ uma variável aleatória qualquer. Então:

\begin{equation*}
\sum_{n=1}^{\infty}P(|X| \ge n) \le E(|X|) \le 1 + \sum_{n=1}^{\infty}P(|X| \ge n)
\end{equation*}

Assim, $X$ é integrável se e somente se $\sum_{n=1}^{\infty}P(|X| \ge n) < \infty$.

::: {.proof name="Prova"}

Seja $x \ge 0$. Tome $[x]$ como a parte inteira de $x$. Então $[|x|] = k$ se $k \le |x| < k+1$. Então:

\begin{align*}
0 \le [|x|] &\le |x| \le [|x|] + 1 \\
&\Downarrow \; \mathbf{E_{2},E_{3}} \\
0 \le E([|x|]) &\le E(|x|) \le E([|x|]) + 1
\end{align*}

Pelo corolário \@ref(cor:esppositiva), como $[|x|]$ é discreta e não-negativa, temos que:

\begin{align*}
E([|x|]) &= \sum_{n=1}^{\infty}P([|x|] \ge n) \\
&= \sum_{n=1}^{\infty}P(|x| \ge n) \le E(|x|) \le \sum_{n=1}^{\infty}P(|x| \ge n) + 1
\end{align*}

:::

### Casos de interesse

**a) (Consistência absoluta)** $\varphi(X) = |X|$:

\begin{equation*}
E(|X|) \ge |E(X)|
\end{equation*}

**b) (Consistência quadrática)** $\varphi(X) = X^{2}$:

\begin{equation*}
E(X^{2}) \ge [E(X)]^{2}
\end{equation*}

**c) (Consistência absoluta de ordem p)** $\varphi(X) = |X|^{p}, p \ge 1$:

\begin{equation*}
E(|X|^{p}) \ge |E(X)|^{p}
\end{equation*}

**Nota**: $\varphi$ só precisa ser convexa (ou côncava) em uma região de probabilidade 1. Por exemplo, se $X$ é uma variável aleatória, tal que $P(X > 0) = 1$, ou o suporte da distribuição de $X$ é $(0, \infty), \varphi(X) = \frac{1}{X}$ é convexa em $(0,\infty) \Rightarrow E\left(\frac{1}{X}\right) \ge \frac{1}{E(X)}$. De modo análogo, se $P(X > 0) = 1$ e $\varphi(X) = \ln(X), \varphi$ é côncava em $(0,\infty)$ logo $E(\ln(X)) \le \ln(E(X))$.

## Esperança de funções de variáveis aleatórias

Seja $X$ uma variável aleatória, $\varphi$ uma função mensurável e $Y = \varphi(X)$. Assim, $Y$ é uma variável aleatória, cuja esperança é $E(Y) = \int ydF_{\varphi(X)}(y) = \int_{0}^{\infty}[1 - F_{\varphi(X)}(y)]dy - \int_{-\infty}^{0}F_{\varphi(X)}(y)dy$.

::: {.theorem}

Se $X$ é uma variável aleatória e $\varphi$ uma função mensurável, com $Y = \varphi(X)$:

\begin{equation*}
E(Y) = E(\varphi(X)) = \int\varphi(x)dF_{X}(x)
\end{equation*}

:::

::: {.proof name="Prova para caso $\varphi(x) = x^{k}$"}

Note que a prova já foi feita para $\varphi(x) = |x|$. Vejamos que a prova é válida para $\varphi(x) = x^{k}$, com $k = 1,2,\ldots$, em 2 casos: $k$ par e $k$ ímpar:

**$\mathbf{k}$ par:**

\begin{align*}
E(X^{k}) &= \int_{0}^{\infty}P\left(X^{k} > t\right)dt \\
&= \int_{0}^{\infty}P\left(X > \sqrt[k]{t}\right)dt + \int_{0}^{\infty}P\left(X < - \sqrt[k]{t}\right)dt \\
&= \int_{0}^{\infty}\left[1 - F_{X}\left(\sqrt[k]{t}\right)\right]dt + \int_{0}^{\infty}F_{X}\left(-\sqrt[k]{t}^{\;-}\right)dt
\end{align*}

Apliquemos as seguintes mudanças de variáveis: $s = t^{\frac{1}{k}}, ds = \frac{1}{k}t^{\frac{1}{k} - 1}dt, dt = \frac{(ds)ks^{k}}{s}, u = -s, du = -ds$:

\begin{align*}
E(X^{k}) &= \int_{0}^{\infty}\left[1-F_{X}(s)\right]ks^{k-1}ds + \int_{0}^{\infty}F_{X}(-s)ks^{k-1}ds \\
&= \int_{0}^{\infty}\left[1-F_{X}(s)\right]ks^{k-1}ds - \int_{-\infty}^{0}F_{X}(u)ku^{k-1}du \\
&= k\left\{\int_{0}^{\infty}[1 - F_{X}(s)]s^{k-1}ds - \int_{-\infty}^{0}F_{X}(u)u^{k-1}du\right\}
\end{align*}

Agora, mostremos que $E(X^{k}) = \int x^{k}dF_{X}(x)$:

\begin{align*}
\int_{-\infty}^{\infty}x^{k}dF_{X}(x) &\stackrel{Def}{=} \int_{0}^{\infty}\left[1 - F_{X}(x)\right]d(x^{k}) - \int_{-\infty}^{0}F_{X}(x)d(x^{k}) \\
&= k\left\{\int_{0}^{\infty}[1 - F_{X}(x)]x^{k-1}dx - \int_{-\infty}^{0}F_{X}(x)x^{k-1}dx\right\} \\
&= E(X^{k})
\end{align*}

:::

**Nota**: A propriedade é também válida para polinômios, visto que a esperança opera de maneira linear.

::: {.example}

Seja $X \sim Exp(\lambda)$, vimos que $E(X) = \frac{1}{\lambda}$:

Calcular $E(X^{2})$:

\begin{align*}
E(X^{2}) = 2\int_{0}^{\infty}xe^{-\lambda x}dx &= \frac{2}{\lambda}\int_{0}^{\infty}x\lambda e^{-\lambda x}dx \\
&= \frac{2}{\lambda}E(X) = \frac{2}{\lambda^{2}}
\end{align*}

Calcular $E(X^{3})$:

\begin{align*}
E(X^{3}) = 3\int_{0}^{\infty}x^{2}e^{-\lambda x}dx &= \frac{3}{\lambda}\int_{0}^{\infty}x^{2}\lambda e^{-\lambda x}dx \\
&= \frac{3}{\lambda}E(X^{2}) = \frac{3}{\lambda^{3}}
\end{align*}

De modo que podemos observar o padrão emergente, e definir $E(X^{k}) = \frac{k!}{\lambda^{k}}$.

:::

## Momentos de uma variável aleatória

**a)** $E\left(\left[X - b\right]^{k}\right)$: $k-$ésimo momento de $X$ em torno de $b$;

**b)** $E\left(X^{k}\right)$: $k-$ésimo momento em torno de 0;

**c)** Se em **(a)**, $b = E(X)$, o momento é central;

**d)** $t>0,E\left(|X|^{t}\right)$: $t-$ésimo momento absoluto de $X$.

::: {.definition #Variancia name="Variância de uma variável aleatória"}

\begin{equation*}
\mathrm{Var}(X) = E\left\{(X - E(X))^{2}\right\} \Longleftrightarrow \mathrm{Var}(X) = E\left(X^{2}\right) - \big{(}E(X)\big{)}^{2}
\end{equation*}

:::

::: {.proposition}

Se $X$ é uma variável aleatória, $f(t) = \left[E(|X|^{t})\right]^{\frac{1}{t}}$ é não-decrescente em $t, t >0$.

:::

::: {.proof name="Prova"}

Devemos provar que, se $0 < s < t, f(s) \le f(t)$ (ou $\left\{E(|X|^{s})\right\}^{\frac{1}{s}} \le \left\{E(|X|^{t})\right\}^{\frac{1}{t}}$). Para tanto, consideremos dois casos: **a)** $E(|X|^{s})<\infty$, **b)** $E(|X|^{s})=\infty$:

**a)** Defina $\varphi(y) = |y|^{\frac{t}{s}}$ (caso $\frac{t}{s} > 1, \varphi$ será convexa). Pela Desigualdade de Jansen:

\begin{align*}
E(\varphi(Y)) &\ge \varphi(E(Y)) \\
E\left(|Y|^{\frac{t}{s}}\right) &\ge \left|E(Y)\right|^{\frac{t}{s}}
\end{align*}

Tome $Y = |X|^{s}$. Substituindo temos:

\begin{align*}
E\left((|X|^{s})^{\frac{t}{s}}\right) &\ge \left|E(|X|^{s})\right|^{\frac{t}{s}} \\
E\left(|X|^{t}\right) &\ge \left\{E(|X|^{s})\right\}^{\frac{t}{s}} \\
\left\{E(|X|^{t})\right\}^{\frac{1}{t}} &\ge \left\{E(|X|^{s})\right\}^{\frac{1}{s}}
\end{align*}

**b)** Como $t > s > 0$, sabemos que $|X|^{s} \le 1 + |X|^{t}$. Como $E(|X|^{s}) = \infty$, então:

\begin{equation*}
\infty = E(|X|^{s}) \le 1 + E(|X|^{t}) = \infty
\end{equation*}

:::

::: {.corollary}

Se $E(|X|^{t}) < \infty \; \forall t \in (0,\infty) \Rightarrow E(|X|^{s}) < \infty \; \forall s$, com $0 < s < t$.

:::

### Propriedades


- $\mathbf{E_{5}}$: Se $X = c$, com $c$ uma constante, $\mathrm{Var}(X) = 0$;
- $\mathbf{E_{6}}$: $\mathrm{Var}(X+b) = \mathrm{Var}(X), \mathrm{Var}(aX + b) = a^{2}\mathrm{Var}(X)$, com $a,b \in \mathbb{R}$;

::: {.proof name="Prova"}

\begin{align*}
\mathrm{Var}(aX + b) &= E\left\{\left[aX + b - E(aX + b)\right]^{2}\right\} \\
&= E\left\{\left[aX + b - aE(X) - b\right]^{2}\right\} \\
&= E\left\{a^{2}\left[X - E(X)\right]^{2}\right\} \\
&= a^{2}E\left\{\left[X - E(X)\right]^{2}\right\} = a^{2}\mathrm{Var}(X)
\end{align*}

:::

- $\mathbf{E_{7}}$**(Desigualdade de Tchebychev)**: Seja $X$ uma variável aleatória, com $X \ge 0$. Para todo $\lambda > 0$:

\begin{equation}
P(X \ge \lambda) \le \frac{E(X)}{\lambda}
(\#eq:desigcheby)
\end{equation}

::: {.proof name="Prova"}

Seja $Y = I_{[X \ge \lambda]}\lambda = \begin{cases}\lambda &,X \ge \lambda \\ 0 &,c.c.\end{cases}$. Por definição, $0 \le Y \le X \Rightarrow E(Y) \le E(X)$ e $E(Y) = \lambda P(X \ge \lambda)$, de modo que:

\begin{equation*}
\lambda P(X \ge \lambda) \le E(X) \Leftrightarrow P(X \ge \lambda) \le \frac{E(X)}{\lambda}
\end{equation*}

:::

### Consequências

**a)** Para todo $\lambda>0$:

\begin{equation*}
P(|X - E(X)| \ge \lambda) \le \frac{\mathrm{Var}(X)}{\lambda^{2}}
\end{equation*}

**b) (Desigualdade de Markov)** Seja $X$ uma variável aleatória, para todo $t$:

\begin{equation}
P(|X| \ge \lambda) \le \frac{E\left(|X|^{t}\right)}{\lambda^{t}}
(\#eq:desigmarkov)
\end{equation}

**c)** Se $Z$ é uma variável aleatória, com $Z \ge 0$ e $E(Z) = 0$:

\begin{equation*}
P(Z=0) = 1 \;\;\;\;(\text{i.e., }Z=0\text{ quase certamente})
\end{equation*}

::: {.proof name="Provas"}

**a)** Se $Y = [X - E(X)]^{2}$, aplicamos $\mathbf{E_{7}}$ usando $\lambda^{2}: P(Y \ge \lambda^{2}) \le \frac{E(Y)}{\lambda^{2}}$. Note que $E(Y) = E([X - E(X)]^{2}) = \mathrm{Var}(X)$. Logo:

\begin{equation*}
P\left(|X - E(X)| \ge \lambda\right) = P(|X - E(X)|^{2} \ge \lambda^{2}) = P(Y \ge \lambda^{2}) \le \frac{E(Y)}{\lambda^{2}} = \frac{\mathrm{Var}(X)}{\lambda^{2}}
\end{equation*}

**b)** Seja $Y = |X|^{t}$, aplicamos $\mathbf{E_{7}}$ a $Y$ e $\lambda^{t}: P(Y \ge \lambda^{t}) \le \frac{E(Y)}{\lambda^{t}}$. Note que $E(Y) = E(|X|^{t})$ e que $P(Y \ge \lambda^{t}) = P(|X|^{t} \ge \lambda^{t}) = P(|X| > \lambda)$. Logo:

\begin{equation*}
P(|X| \ge \lambda) \le \frac{E(|X|^{t})}{\lambda^{t}}
\end{equation*}

**c)** $Z = 0$ quase certamente, usamos $\mathbf{E_{7}}$ na variável $Z$ e em $\lambda = \frac{1}{n}$, então:

\begin{equation*}
P\left(Z \ge \frac{1}{n}\right) \le E(Z).n \stackrel{Hip}{=}0
\end{equation*}

Temos que $[Z > 0] = \bigcup_{n}\left[Z \ge \frac{1}{n}\right]$, de modo que:

\begin{equation*}
P(Z > 0) = P\left(\bigcup_{n}\left[Z \ge \frac{1}{n}\right]\right) = \lim_{n \to \infty}P\left(Z \ge \frac{1}{n}\right) = 0 \Rightarrow P(Z = 0) = 1 - P(Z > 0) = 1
\end{equation*}

:::

**Nota**: Se $X$ é uma variável tal que $\mathrm{Var}(X) = 0$, temos que $\mathrm{Var}(X) = 0 \Leftrightarrow E([X - E(X)]^{2}) = 0$, ou seja, se definirmos $Z = [X - E(X)]^{2}, Z \ge 0$ e $E(Z) = 0$. Logo, por **c)**, $P(Z = 0) = 1$, ou seja, $P([X - E(X)]^{2} = 0) = 1 \Leftrightarrow P(X = E(X)) = 1$, ou seja, $X = E(X)$ quase certamente.

::: {.example}

Se $X$ e $Y$ são variáveis aleatórias tais que $E(|X|^{t}) < \infty$ e $E(|Y|^{t}) < \infty$, então $E(|X + Y|^{t}) < \infty$:

**(i)** A finitude de $E(|X|^{t})$ leva à finitude de $E(|aX|^{t})$;

**(ii)** Se $X$ e $Y$ forem integráveis (com $t = 1$), então $X + Y$ é integrável. Se $X$ e $Y$ tem variâncias finitas $(t = 2)$, então $X + Y$ tem variância finita.

:::

::: {.proposition}

Seja $X$ uma variável aleatória integrável e $\mu = E(X) \Rightarrow \mu$ minimiza $E([X - c]^{2})$, com $c \in \mathbb{R}$.

:::

::: {.proof name="Prova"}

Temos que $(X - c)^{2} = (X - \mu + \mu - c)^{2} = (X - \mu)^{2} + 2(\mu - c)(X - \mu) + (\mu - c)^{2}$. Logo, pelas propriedades lineares do valor esperado:

\begin{align*}
E([X - c]^{2}) &= E([X - c]^{2}) + 2(\mu - c)E(X - \mu) + (\mu - c)^{2} \\
&= \mathrm{Var}(X) + (\mu - c)^{2}
\end{align*}

:::

::: {.proposition}

Seja $X$ uma variável aleatória e $m$ sua mediana. Assim, $m$ minimiza $E(|X - c|), c \in \mathbb{R}$. Ou seja:

\begin{equation*}
E(|x - m|) = \min_{c \in \mathbb{R}}E(|X - c|)
\end{equation*}

:::

::: {.proof name="Prova"}

Considere a definição de mediana: $P(X \le m) = P(X > m) = \frac{1}{2}$. Suponha que $X$ é integrável, logo $X - c$ também o será para todo $c$ constante real. Vamos ver que, com $m < c$ (o caso em que $m > c$ segue analogamente):

- $X \le m \Rightarrow |X - c| - |X - m| = \lambda$, onde $\lambda = c - m$;
- $X > m \Rightarrow |X - c| - |X - m| \ge \lambda$.

Seja $c$ tal que $m < c$. Defina $\lambda = c - m > 0$. Então:

- Se $x \le m \Rightarrow |x - c| = |x - m| + \lambda \Rightarrow |x - c| - |x - m| = \lambda$;
- Se $x > m$ e $x > c$ (os casos intermediários são decorrências), então $x > c \Rightarrow \lambda + |x - c| = |x - m| \Rightarrow |x - c| - |x - m| \ge - \lambda$.

Defina $Y = |X - c| - |X - m| = \begin{cases}
\lambda & ,\text{se } X \le m, \\
y & ,\text{se }X > m.
\end{cases}$. Assim, temos que $y \ge -\lambda$, e que $Y = \begin{cases}
\lambda & ,P(X \le m) \ge \frac{1}{2}, \\
y &,P(X > m) \le \frac{1}{2}
\end{cases}$. Logo, $Y \ge \lambda I_{[X \le m]} - \lambda I_{[X > m]} \Rightarrow E(Y) \ge \lambda E(I_{[X \le m]}) - \lambda E(I_{[X > m]}) = \lambda P(X \le m) - \lambda P(X > m) \ge 0$.

Como $E(Y) \ge 0 \Rightarrow E(|X - c|) \ge E(|x - m|) \; \forall c$, com $m < c$.

:::

## Esperanças e funções de vetores

::: {.theorem}

Seja $\underbar{X} = (X_{1},\ldots,X_{n})$ um vetor aleatório e $\varphi:\mathbb{R}^{n} \to \mathbb{R}$ mensurável. Então:

\begin{align*}
E(\varphi(\underbar{X})) = \int ydF_{\varphi\left(\underbar{X}\right)}(y) &= \int_{\mathbb{R}^{n}}\varphi(\underbar{x})dF_{\underbar{X}}(\underbar{x}) \\
&= \int \cdots \int \varphi(x_{1},\ldots,x_{n})dF_{\underbar{X}}(x_{1},\ldots,x_{n})
\end{align*}

Caso discreto: Seja $\underbar{X}$ discreto, tomando valores $\underbar{X}_{i} = (x_{i1},\ldots,x_{in})$, com probabilidade $P(\underbar{X}_{i}) = \sum_{i}P(x_{i}) = 1$. Então:

\begin{equation*}
E(\varphi(\underbar{X})) = \sum_{i}\varphi(\underbar{x}_{i})P(\underbar{x}_{i})
\end{equation*}

Caso contínuo: Seja $\underbar{X}$ contínuo, com densidade $f(x_{1},\ldots,x_{n})$. Então:

\begin{equation*}
E(\varphi(\underbar{X})) = \int \cdots \int f(x_{1},\ldots,x_{n})dx_{1}\ldots dx_{n}
\end{equation*}

:::

::: {.example}

Lembrando a propriedade $E_{3}: E(X + Y) = E(X) + E(Y)$ desde que existam $E(X)$ e $E(Y)$. Seja $\varphi(x,y) = x + y$ e defina $\varphi_{1}(x,y) = x$ e $\varphi_{2}(x,y) = y$. Teremos pelo teorema que:

\begin{align*}
E(X + Y) = E(\varphi(x,y)) = \int \int(x+y)dF_{X,Y}(x,y) &= \int\int x dF_{X,Y}(x,y) + \int\int y dF_{X,Y}(x,y) \\
&= E(\varphi_{1}(x,y)) + E(\varphi_{2}(x,y)) \\
&= E(X) + E(Y)
\end{align*}

:::

Se $\{X_{i}\}_{i=1}^{n}$ é conjuntamente independente, com densidades $f_{1},\ldots,f_{n}$, sendo a densidade conjunta dada por $f = \prod_{i = 1}^{n}f_{i}$, então:

\begin{align*}
E(\varphi(\underbar{X})) &= \int \cdots \int \varphi(x_{1},\ldots,x_{n})f_{1}(x_{1})\ldots f_{n}(x_{n})dx_{1}\ldots dx_{n} \\
&= \int \cdots \int \varphi(x_{1}, \ldots, x_{n})dF_{X_{1}}(x_{1})\ldots dF_{X_{n}}(x_{n})
\end{align*}

::: {.proposition}

Sejam $\{X_{i}\}_{i=1}^{n}$ conjuntamente independentes e integráveis. Então:

\begin{equation*}
E\left(\prod_{i=1}^{n}X_{i}\right) = \prod_{i=1}^{n}E(X_{i})
\end{equation*}

:::

::: {.proof name="Prova (para $n = 2$)"}

Seja $\varphi(X,Y) = X.Y$:

\begin{align*}
E(X.Y) = E(\varphi(X,Y)) &= \int \int \varphi(x,y)dF_{X}(x)dF_{Y}(y) \\
&= \int[y.xdF_{X}(x)]dF_{Y}(y) \\
&= \int yE(X)dF_{Y}(y) \\
&= E(X)\int ydF_{Y}(y) = E(X)E(Y)
\end{align*}

:::

::: {.definition}

A covariância entre $X$ e $Y$ será definida por:

\begin{equation*}
\mathrm{Cov}(X,Y) = E\Big[\big(X - E(X)\big)\big(Y - E(Y)\big)\Big]
\end{equation*}

Sempre que $X$ e $Y$ sejam integráveis. Assim:

\begin{align*}
\mathrm{Cov}(X,Y) &= E\{XY - E(Y)X - E(X)Y + E(X)E(Y)\} \\
&= E(XY) - E(X)E(Y) - E(X)E(Y) + E(X)E(Y) \\
&= E(XY) - E(X)E(Y)
\end{align*}

Note que $X$ e $Y$ podem ter $Cov(X,Y) = 0$ e mesmo assim $X \not\perp Y$.

:::

**Notas**:

- A existência da covariância entre variáveis integráveis depende da existência de $E(XY)$;
- $Cov(X,Y) = 0$ é interpretado como "$X$ e $Y$ são não-correlacionados";
- Há casos onde $Cov(X,Y) = 0$ implica independência, como na Normal Bivariada, por exemplo.

::: {.proposition}

Sejam $X_{1},\ldots,X_{n}$ variáveis aleatórias integráveis tais que $\mathrm{Cov}(X_{i},X_{j}) = 0 \; \forall i \neq j$. Então

\begin{equation*}
\mathrm{Var}\left(\sum_{i=1}^{n}X_{i}\right) = \sum_{i=1}^{n}\mathrm{Var}(X_{i})
\end{equation*}

:::

::: {.proof name="Prova"}

\begin{align*}
\mathrm{Var}\left(\sum_{i=1}^{n}X_{i}\right) &= E\left\{[X_{1} + \ldots X_{n}] - E[X_{1} + \ldots + X_{n}]^{2}\right\} \\
&= E\left\{\sum_{i=1}^{n}[X_{i} - E(X_{i})]^{2} + 2 \sum_{i < j}(X_{i} - E(X_{i}))(X_{j} - E(X_{j}))\right\} \\
&= \sum_{i=1}^{n}E\left[(X_{i} - E(X_{i}))^{2}\right] + 2 \sum_{i < j}E[(X_{i} - E(X_{i}))(X_{j} - E(X_{j}))] \\
&= \sum_{i=1}^{n}\mathrm{Var}(X_{i}) + 2 \sum_{i < j}\mathrm{Cov}(X_{i},X_{j}) \\
&= \sum_{i=1}^{n}\mathrm{Var}(X_{i})
\end{align*}

:::

::: {.corollary}

Sejam $X_{1},\ldots,X_{n}$ variáveis aleatórias independentes e integráveis. Então:

\begin{equation*}
\mathrm{Var}\left(\sum_{i=1}^{n}X_{i}\right) = \sum_{i=1}^{n}\mathrm{Var}(X_{i})
\end{equation*}

:::

::: {.definition #corrpearson}

Para $X$ e $Y$ variáveis aleatórias, o coeficiente de correlação de Pearson é definido por:

\begin{equation*}
\rho_{X,Y} = \frac{\mathrm{Cov}(X,Y)}{\sigma_{X}\sigma_{Y}}
\end{equation*}

Com $\sigma_{X} = \sqrt{\mathrm{Var(X)}} \text{ e } \sigma_{Y} = \sqrt{\mathrm{Var(Y)}}$, sempre que $\mathrm{Var}(X)$ e $\mathrm{Var}(Y)$ sejam finitas e maiores que 0.

:::

::: {.proposition}

Sob os supostos da definição \@ref(def:corrpearson):

- **a)** $-1 \le \rho_{X,Y} \le 1$;
- **b)** $\rho_{X,Y} = 1 \Leftrightarrow P(Y = aX + b) = 1$, para algum $a > 0$ e $b \in \mathbb{R}$;
- **c)** $\rho_{X,Y} = -1 \Leftrightarrow P(Y = aX + b) = 1$, para algum $a < 0$ e $b \in \mathbb{R}$.

:::

::: {.proof name="Prova"}

Note primeiramente que $\mathrm{Cov}(X,Y) = E\left\{(X - E(X))(Y - E(Y))\right\}$, logo:

\begin{equation*}
\rho_{X,Y} = \frac{\mathrm{Cov}(X,Y)}{\sigma_{X}\sigma_{Y}} = E\left\{\frac{(X - E(X))}{\sigma_{X}}\frac{(Y - E(Y))}{\sigma_{Y}}\right\}
\end{equation*}

Observe que $E\left(\frac{X - E(X)}{\sigma_{X}}\right) = 0$ e $\mathrm{Var}\left(\frac{X - E(X)}{\sigma_{X}}\right) = 1$, e analogamente para $Y$. Assim:

**a)**

\begin{align*}
0 &\le \left(\frac{(X - E(X))}{\sigma_{X}} - \frac{(Y - E(Y))}{\sigma_{Y}}\right)^{2} \\
0 &\le E\left\{\left(\frac{(X - E(X))}{\sigma_{X}} - \frac{(Y - E(Y))}{\sigma_{Y}}\right)^{2}\right\} \\
0 &\le E\left(\left[\frac{X - E(X)}{\sigma_{X}}\right]^{2}\right) + E\left(\left[\frac{Y - E(Y)}{\sigma_{Y}}\right]^{2}\right) - \frac{2}{\sigma_{X}\sigma_{Y}}E((X - E(X))(Y - E(Y))) \\
0 &\le \frac{\mathrm{Var}(X)}{\sigma_{X}^{2}} + \frac{\mathrm{Var}(Y)}{\sigma_{Y}^{2}} - \frac{2\mathrm{Cov}(X,Y)}{\sigma_{X}\sigma_{Y}} \\
0 &\le 2 - 2\rho_{X,Y} \\
\rho_{X,Y} \le 1
\end{align*}

Tomando a diferença ao invés da soma, chegamos que $\rho_{X,Y} \ge -1$.

**b)**

Suponha $\rho_{X,Y} = 1 \Leftrightarrow E\left\{\left[\frac{X - E(X)}{\sigma_{X}} - \frac{Y - E(Y)}{\sigma_{Y}}\right]^{2}\right\} = 0$. Pela propriedade $E_{7}$, temos que:

\begin{equation*}
P\left(\frac{X - E(X)}{\sigma_{X}} = \frac{Y - E(Y)}{\sigma_{Y}}\right) = 1
\end{equation*}

Ou seja, $Y \stackrel{q.c}{=} E(Y) + \frac{\sigma_{Y}}{\sigma_{X}}(X - E(X))$, então $a = \frac{\sigma_{Y}}{\sigma_{X}}>0, b = E(Y) - \frac{\sigma_{Y}}{\sigma_{X}}E(X)$.

:::

**Nota**: Se $P(Y = aX + b) = 1$, sendo $a \neq 0$, pelo desenvolvimento da prova de **(a)**, temos que:

\begin{align*}
\rho_{X,Y} &= E\left\{\left(\frac{X - E(X)}{\sigma_{X}}\right)\left(\frac{aX + b - aE(X) - b}{\sqrt{a^{2}\sigma_{X}^{2}}}\right)\right\} \\
&= \frac{a}{|a|}E\left\{\left[\frac{X - E(X)}{\sigma_{X}}\right]^{2}\right\} \\
&= \frac{a}{|a|} = \mathrm{sgn}(a) = \pm 1
\end{align*}

## Convergência

::: {.theorem name="Teorema da convergência monótona"}

Sejam $X_{1},X_{2},\ldots$ e $X$ variáveis aleatórias em $(\Omega,\mathcal{A},P)$. Se $0 \le X_{n} \underset{n \to \infty}{\uparrow}X$ (ou seja, $X_{n}(\omega) \ge 0 ,\forall \omega \in \Omega$ e $X_{n}(\omega)\underset{n \to \infty}{\uparrow}X(\omega),\forall \omega \in \Omega$). Então $E(X_{n}) \underset{n \to \infty}{\uparrow} E(X)$.

:::

::: {.proof name="Prova"}

Como $X_{n} \underset{n \to \infty}{\uparrow} X, X_{n} \ge 0$, por $E_{2}$ temos que $0 \le E(X_{n}) \le E(X)$. Devemos então provar que $\forall \epsilon > 0, \lim_{n \to \infty}E(X_{n}) \ge E(X) - \epsilon$ (ou seja, $\forall \epsilon > 0 \;\exists\; n_{0}(\epsilon) : E(X_{n}) \ge E(X) - \epsilon, \forall n: n \ge n_{0}(\epsilon)$).

Defina $Y = \sum_{n=0}^{\infty}n\epsilon I_{B_{n}}$, onde $B_{n} = [x\epsilon < X \le (n+1)\epsilon], n = 0,1,\ldots$. Assim:

\begin{align*}
n=0 &: B_{0} = [0 < X \le \epsilon] \\
n=1 &: B_{1} = [\epsilon < X \le 2\epsilon] \\
n=2 &: B_{2} = [2\epsilon < X \le 3\epsilon] \\
\vdots &\; \\
Y &= \begin{cases}
n\epsilon &, \text{se } n\epsilon < X \le (n+1)\epsilon \\
0 &, \text{se }X = 0
\end{cases}
\end{align*}

Temos então que mostrar que $X - \epsilon \le Y \le X$. Como $Y = n\epsilon < X$, o lado direito é dado diretamente. Para o lado esquerdo temos que:

\begin{equation*}
X \le (n+1)\epsilon = n\epsilon + \epsilon \Leftrightarrow X - \epsilon \le n\epsilon = Y \Rightarrow E(X) - \epsilon \le E(Y) \le E(X)
\end{equation*}

Note que, se $E(Y) \le \lim_{n \to \infty}E(X_{n})$, então teremos provado o resultado. Seja $A_{k} = [X_{k} \ge Y]$, com $k$ grande o suficiente. Se $\omega$ é tal que $X_{k}(\omega) \ge Y(\omega) \Rightarrow X_{k+1}(\omega) \ge Y(\omega) \Rightarrow A_{k} \subseteq A_{k+1} \subseteq A_{k+2} \subseteq \ldots$.

Formalmente, pelo limite $X_{k} \uparrow X$ se $k$ é suficientemente grande, $X_{k}(\omega) \ge Y(\omega)$. Assim, $\bigcup A_{k} = [Y \le X] = \Omega$, e:

\begin{equation*}
YI_{k} = \begin{cases}
Y(\omega) &, \text{se }\omega \in A_{k} \\
0 &, \text{se } \omega \not\in A_{k}
\end{cases} = \begin{cases}
n\epsilon &, \text{se }\omega \in B_{n} \cap A_{k}, n=0,1,\ldots \\
0 &, \text{se } \omega \not\in \bigcup_{n=0}^{\infty}B_{n}\cap A_{k}
\end{cases}
\end{equation*}

Logo, $0 \le YI_{k} \le X_{k} \Rightarrow 0 \le E(YI_{k}) \le E(X_{k})$. Assim, $E(YI_{k})$ será:

\begin{equation*}
E(YI_{k}) = \sum_{n=0}^{\infty}n\epsilon P(B_{n} \cap A_{k}) \ge \sum_{n=0}^{m}n\epsilon P(B_{n} \cap A_{k})
\end{equation*}

Logo, $\lim_{k \to \infty}E(X_{k}) \ge \lim_{k \to \infty} \sum_{n=0}^{m}n\epsilon P(B_{n} \cap A_{k}) = \sum_{n=0}^{m}n\epsilon P(B_{n}), \forall m$. Como $m$ é arbitrário, $\lim_{k \to \infty}E(X_{k}) \ge \sum_{n=0}^{m}n\epsilon P(B_{n}) = E(Y)$.

:::

::: {.theorem name="Teorema da convergência dominada"}

Sejam $Y,X_{1},X_{2},\ldots,X$ variáveis aleatórias em $(\Omega, \mathcal{A},P)$, tais que $Y$ é integrável, $|X_{n}| \le Y \; \forall n$ e $X_{n} \to X$ (ou seja, dado $\omega: X_{n}(\omega) \xrightarrow[n \to \infty]{}X(\omega)$). Então $X$ e $X_{n}$ são integráveis e $E(X_{n}) \xrightarrow[n \to \infty]{}E(X)$.

:::

::: {.proof name="Prova"}

Por hipótese, temos a integrabilidade de $X$ e $X_{n}$, tais que:

\begin{equation*}
|X| = \lim_{n \to \infty}|X_{n}| \stackrel{\mathrm{hip}}{\le} Y
\end{equation*}

Assim, $X$ e $X_{n}$ são dominadas, então por $E_{2}$, $X$ e $X_{n}$ são integráveis. Defina $Y_{n} = \inf_{k \ge n}X_{k}$. Tomar o ínfimo provoca a sequência a se movimentar pela esquerda, de modo que:

\begin{equation*}
X(\omega) \stackrel{\mathrm{hip}}{=}\lim_{n \to \infty}X_{n}(\omega) = \lim_{n \to \infty}\inf X_{n}(\omega) = \lim_{n \to \infty}\left(\inf_{k \ge n}X_{k}(n)\right) = \lim_{n \to \infty}Y_{n}
\end{equation*}

E por definição de $Y_{n}:Y_{n} \uparrow X \Rightarrow (Y_{n} + Y) \underset{n \to \infty}{\uparrow} (X + Y)$. Aplicando o teorema da convergência monótona, temos que $|X_{n}| \le Y \;\forall n \Rightarrow -Y \le X_{n}$, logo $\inf_{k \ge n}X_{k} \ge -Y \Rightarrow Y_{n} \ge -Y \Rightarrow Y_{n} + Y \ge 0$. Logo:

\begin{equation*}
E(Y_{n} + Y) \underset{n \to \infty}{\uparrow} E(X + Y)
\end{equation*}

Defina $Z_{n}(\omega) = \sup_{k \ge n}X_{k}(\omega)$. Note que $Z_{n}(\omega) \underset{n \to \infty}{\downarrow} X(\omega)$, logo $(Y - Z_{n}) \underset{n \to \infty}{\uparrow} (Y - X)$. Note que $|X_{n}| \le Y, \forall n \Rightarrow X_{n} \le Y$, de modo que:

\begin{equation*}
\sup_{k \ge n}X_{k} \le Y \Rightarrow Z_{n} \le Y \Rightarrow 0 \le Y - Z_{n}
\end{equation*}

Agora que temos a positividade e a monotonia do crescimento, utilizamos o teorema da convergência monótona, de modo que:

\begin{equation*}
E(Y - Z_{n}) \underset{n \to \infty}{\uparrow} E(Y - X) \Rightarrow E(Z_{n}) \underset{n \to \infty}{\downarrow} E(X)
\end{equation*}

Juntando as convergências de $Y_{n}$ e $Z_{n}$:

\begin{equation*}
Y_{n} = \inf_{k \ge n}X_{k} \le X_{n} \le \sup_{k \ge n}X_{k} = Z_{n} \Longrightarrow E(Y_{n}) \le E(X_{n}) \le E(Z_{n})
\end{equation*}

De modo que $E(X_{n}) \to E(X)$.

:::

### Observações sobre o teorema da convergência dominada

1. Há casos nos quais $X_{n} \to X$, no entanto $E(X_{n}) \not\to E(X)$;

::: {.example}

$X \sim \mathrm{Cauchy} \rightarrow f(x) = \frac{1}{\pi(1 + x^{2})}$. Sabemos que $E(X) = \infty$. Seja $X_{n} = I_{[-n \le x \le n]} = \begin{cases}
X &, -n \le x \le n \\
0 &, |x| > n
\end{cases}$. Então:

**i)** $X_{n} \underset{n \to \infty}{\rightarrow} X$;

**ii)** Cada $X_{n}$ é limitado pois $|X_{n}| \le n$, ou seja, podemos computar $E(X_{n})$;

**iii)** $E(X_{n}) = 0 \;\forall n$ fixo, pois $X_{n}$ é simétrica em torno de $0$, de modo que $E(X_{n}) \not \to E(X) = \infty$.

:::

2. Levemos em consideração que se $0 < s < t \Rightarrow |X|^{s} \le 1 + |X|^{t}$. Então, se $X$ é uma variável aleatória tal que $E(|X|^{t}) < \infty$ para $t > 0 \Rightarrow g(s) = E(|X|^{s})$ é contínua para todo $s \in (0,t]$. Basta ver que se $s_{n} \to s \Rightarrow g(s_{n}) \to g(s)$. Note que $|X|^{s_{n}} \le |X|^{t} + 1 \Rightarrow E(|X|^{s_{n}}) \le E(|X|^{t}) + 1 < \infty$. Assim, pelo teorema da convergência dominada:

\begin{equation*}
E(|X|^{s_{n}}) \to E(|X|^{s})
\end{equation*}

3. **(Teorema de Arzelà)** Sejam $f,f_{1},f_{2},\ldots$, funções reais (borel-mensuráveis) definidas em $[a,b]$, com $a < b$ e integráveis a Riemann. Se $f_{n} \to f$ e se $|f_{n}| \le M \; \forall n$, então:

\begin{equation*}
\int_{a}^{b}f_{n}(x)dx \to \int_{a}^{b}f(x)dx
\end{equation*}

::: {.proof name="Prova"}

Considere $Omega = [a,b]$ e $\mathcal{A} = \mathcal{B}_{[a,b]}$ (ou seja, a $\sigma-$álgebra dos borelianos em $[a,b]$). Defina $\forall \omega \in \Omega$:

\begin{align*}
X_{n}(\omega) &= (b-a)f_{n}(\omega) \\
X(\omega) &= (b-a)f(\omega)
\end{align*}

Já que $f_{n} \to f$ por hipótese, então $X_{n}(\omega) \to X(\omega) \; \forall \omega \in \Omega$. Também $|f_{n}| \le M \Rightarrow (a-b)M \ge |X_{n}|$, logo $\{X_{n}\}$ é uma sequência de variáveis aleatórias integráveis, e sob o teorema da convergência dominada:

\begin{equation*}
E(X_{n}) \underset{n \to \infty}{\longrightarrow}E(X)
\end{equation*}

:::

4. **(Convergência de séries)** Se $a_{mn} \ge 0$ para $m,n = 1,2,\ldots$ e $a_{mn} \underset{n \to \infty}{\uparrow}a_{m} \; \forall m$, então:

\begin{equation*}
\sum_{m=1}^{\infty}a_{mn} \uparrow \sum_{m=1}^{\infty}a_{m}
\end{equation*}

::: {.proof name="Prova"}

Podemos escrever $a_{mn} = X_{n}(m)p_{m}$ onde $X_{n}(m) = \frac{a_{mn}}{p_{m}}, p_{m} = p(m) \; \forall m, m = \{1,2,\ldots\}$, sendo $p_{m}$ tal que $\sum_{m=1}^{\infty}p_{m} = 1$. Analogamente $a_{m} = X(m)p_{m}$, e sendo $p_{m} \ge 0 \; \forall m$, como $a_{mn} \ge 0$ e $p_{m} \ge 0 \Rightarrow X_{n}(m) \ge 0$. Já que $a_{mn} \underset{n \to \infty}{\uparrow}a_{m} \Rightarrow 0 \le X_{n}(m) \underset{n \to \infty}{\uparrow} X(m), \forall m$. Logo, pelo teorema da convergência monótona, $E(X_{n}) \underset{n \to \infty}{\uparrow} E(X)$, e assim:

\begin{align*}
E(X_{n}) &= \sum_{m=1}^{\infty}X_{n}(m)p_{m} = \sum_{m=1}^{\infty}a_{mn} \\
E(X) &= \sum_{m=1}^{\infty}X(m)p_{m} = \sum_{m=1}^{\infty}a_{m}
\end{align*}

Ou seja, $\sum_{m=1}^{\infty}a_{mn} \underset{n \to \infty}{\uparrow} \sum_{m=1}^{\infty}a_{m}$.

:::

\newpage

## Exercícios

TODO

\newpage
