---
title: "Notas de Aula - Capítulo 3"
subtitle: "Probabilidade"
author: "Caio Gomes Alves"
date: "`r format(Sys.Date(), '%d/%m/%Y')`"
header-includes:
  - \usepackage{amsmath}
  - \usepackage{tikz}
  - \usepackage{pgfplots}
  - \usepackage{caption}
  - \usepackage{subcaption}
  - \usepackage{cancel}
  - \usepackage{mathtools}
  - \usepackage{annotate-equations}
output:
  bookdown::pdf_document2:
    toc: false
---

# Esperança

## Definição

::: {.definition}

Se $X$ é uma variável aleatória com distribuição $F$, a esperança de $X$ é definida por $E(X = \int_{-\infty}^{\infty}x dF(x)$, sempre que a integral estiver bem definida.

:::

**Convenção**: Se $E(X) < \infty$, então $X$ é integrável.

**Nota**: $\int_{-\infty}^{\infty}xdF(x)$ é bem definida se $\int_{0}^{\infty}xdF(x)$ ou $\int_{-\infty}^{0}xdF(x)$ for finita, já que $\int_{-\infty}^{\infty}xdF(x) = \underbrace{\int_{-\infty}^{0}xdF(x)}_{\mathclap{\mathbf{I} \le 0}} + \underbrace{\int_{0}^{\infty}xdF(x)}_{\mathclap{\mathbf{II} \ge 0}}$. Assim, podemos separar em quatro casos:

1. Se **I** e **II** são finitos, então $X$ é integrável;
2. Se **I** é finito e $\mathbf{II} = +\infty$, então $E(X) = +\infty$;
3. Se **II** é finito e $\mathbf{I} = -\infty$, então $E(X) = -\infty$;
4. Se $\mathbf{I} = -\infty$ e $\mathbf{II} = +\infty$, então $E(X)$ é indefinida.

**Propriedade**: $E(|X|) = \int |x| dF(x)$. Logo, $X$ é integrável se e somente se $E(|X|) < \infty$.

::: {.example}

$X \sim U(0,1),\; Y = \min\left(X,\frac{1}{2}\right)$:

\begin{align*}
P\left(Y = \frac{1}{2}\right) &= P\left(X > \frac{1}{2}\right) = 1 - F_{X}\left(\frac{1}{2}\right) = 1 - \frac{1}{2} = \frac{1}{2} = P_{Y}\left(Y = \frac{1}{2}\right)\\
E(Y) = \int_{-\infty}^{\infty}ydF(y) &= \int_{0}^{1/2}y.1dy + \frac{1}{2}P_{Y}\left(Y=\frac{1}{2}\right) \\
&= \frac{y^{2}}{2}\bigg{|}_{0}^{1/2} + \frac{1}{4} \\
&= \frac{1}{8} + \frac{1}{2} = \frac{3}{8}
\end{align*}

:::

::: {.proposition #separespe}

$E(X) = \int_{0}^{\infty}(1 - F(x))dx - \int_{-\infty}^{0}F(x)dx$. Disso, temos que:

- **a)** $\int_{0}^{\infty}xdF(x) = \int_{0}^{\infty}(1 - F(x))dx$;
- **b)** $\int_{-\infty}^{0}xdF(x) = - \int_{-\infty}^{0}F(x)dx$;

:::


::: {.proof name="Prova"}

Vejamos **(a)**: considere que $d(xF(x)) = F(x)dx + xd(F(x)) \Rightarrow xd(F(x)) = d(xF(x)) - F(x)dx$. Seja um $b > 0$:

\begin{align*}
\int_{0}^{b}xdF(x) &= \int_{0}^{b}d(xF(x)) - \int_{0}^{b}F(x)dx \\
&= xF(x)\bigg{|}_{0}^{b} - \int_{0}^{b}F(x)dx \\
&= bF(b) - \int_{0}^{b}F(x)dx \\
&= \int_{0}^{b}[F(b) - F(x)]dx
\end{align*}

Note que $\int_{0}^{b}xdF(x) \le \int_{0}^{\infty}[1 - F(x)]dx, \; \forall b>0$. Basta notar que $F(b) - F(x) \le 1 - F(x)$ e que $\int_{0}^{b}[1 - F(x)]dx \le \int_{0}^{\infty}[1 - F(x)]dx$. Logo:

\begin{equation*}
\int_{0}^{\infty}xdF(x) = \lim_{b \to \infty}\int_{0}^{b}xdF(x) \le \int_{0}^{\infty}[1 - F(x)]dx \Rightarrow \int_{0}^{\infty}xdF(x) \le \int_{0}^{\infty}[1 - F(x)]dx
\end{equation*}

Considere $\lambda > 0$ e $b > 0$, tais que:

\begin{align*}
\int_{0}^{b}[F(b) - F(x)]dx \ge \int_{0}^{\lambda}[1 - F(x)]dx &= \int_{0}^{\lambda}[F(b) - 1]dx + \int_{0}^{\lambda}[1 - F(x)]dx \\
&= \lambda[F(b) - 1] + \int_{0}^{\lambda}[1 - F(x)]dx \\
\int_{0}^{b}[F(b) - F(x)]dx &\ge \lambda[F(b) - 1] + \int_{0}^{\lambda}[1 - F(x)]dx
\end{align*}

Logo, como $\int_{0}^{\infty}xdF(x) = \lim_{b \to \infty}\int_{0}^{b}[F(b) - F(x)]dx \ge \lim_{b \to \infty}\{\lambda[F(b) - 1] + \int_{0}^{\lambda}[1 - F(x)]dx\} = \int_{0}^{\lambda}[1 - F(x)]dx$. Assim:

\begin{equation*}
\int_{0}^{\infty}xdF(x) \ge \lim_{\lambda \to \infty}\int_{0}^{\lambda}[1 - F(x)]dx = \int_{0}^{\infty}[1 - F(x)]dx
\end{equation*}

E como $\int_{0}^{\infty}xdF(x) \le \int_{0}^{\infty}[1 - F(x)]dx$, temos que $\int_{0}^{\infty}xdF(x) = \int_{0}^{\infty}[1 - F(x)]dx$

:::

::: {.corollary #esppositiva}

Se $X$ é tal que $X(\omega) \ge 0 \; \forall \omega \in \mathbb{R} \Rightarrow E(X) = \int_{0}^{\infty}[1 - F(x)]dx = \int_{0}^{\infty}P(X \ge x)dx$.

:::

::: {.example}

Seja $X \sim Exp(\lambda)$, qual a $E(X)$? Como o suporte de $X$ é $(0,\infty)$, aplica-se o corolário anterior, de modo que:

\begin{align*}
F_{X}(x) &= 1 - e^{-\lambda x} \Leftrightarrow P(X > x) = e^{-\lambda x} \\
E(X) &= \int_{0}^{\infty}e^{-\lambda x}dx = -\frac{1}{\lambda}e^{-\lambda x}\bigg{|}_{0}^{\infty} = \frac{1}{\lambda}
\end{align*}

:::

**Nota**: Suponha $X$ discreta e $X(\omega) \ge 0 \; \forall \omega$. Então:

\begin{align*}
E(X) = \sum_{n=0}^{\infty}P(X > n) &= \sum_{n=0}^{\infty}P(X \ge n+1) \\
&= \sum_{n=1}^{\infty}P(X \ge n)
\end{align*}

::: {.example}

Considere o lançamento de uma moeda até a 1ª cara. Suponha $p=$ probabilidade de cara e $(1 - p) =$ probabilidade de coroa, e $X =$ número de lançamentos até a primeira cara. Tome o evento $[X \ge n]$, logo:

\begin{equation*}
E(X) = \sum_{n=1}^{\infty}(1 - p)^{n-1} = \sum_{n=0}^{\infty}(1 - p)^{n} = \frac{1}{p}
\end{equation*}

:::

**Nota**: Sendo $X$ uma variável aleatória, temos pelo corolário \@ref(cor:esppositiva) que:

\begin{align*}
E(|X|) &= \int_{0}^{\infty}P(|X| > x)dx \\
&= \int_{0}^{\infty}\big[P(X > x) + P(X < -x)\big]dx \\
&= \int_{0}^{\infty}P(X > x)dx + \int_{0}^{\infty}P(X < -x)dx \\
&= \int_{0}^{\infty}(1-F(x))dx + \int_{0}^{\infty}F((-x)^{-})dx
\end{align*}

Onde $F((-x)^{-}) = \lim_{u \uparrow -x}F(u)$, que caso $F$ seja contínua, coincide com $F(-x)$. Logo:

\begin{equation*}
E(|X|) = \int_{0}^{\infty}(1 - F(x))dx + \int_{0}^{\infty}F(-x)dx
\end{equation*}

Já que $F$ pode ser descontínua em uma coleção enumerável de pontos. Agora, tomando a transformação de variável $y = -x \Leftrightarrow dy = -dx$:

\begin{align*}
E(|X|) &= \int_{0}^{\infty}(1 - F(x))dx + \int_{-\infty}^{0}F(y)dy \\
&= \int_{0}^{\infty}(1 - F(x))dx + \int_{-\infty}^{0}F(x)dx
\end{align*}

Utilizando os resultados **a** e **b** da proposição \@ref(prp:separespe), temos que:

\begin{align*}
E(|X|) &= \int_{0}^{\infty}xdF(x) - \int_{-\infty}^{0}xdF(x) \\
&= \int_{0}^{\infty}|x|dF(x) + \int_{-\infty}^{0}|x|dF(x) \\
&= \int_{-\infty}^{\infty}|x|dF(x)
\end{align*}

Onde $F$ é a acumulada de $X$, ao invés de $|X|$. Assim, a integrabilidade de $X$ depende da finitude de $\int_{0}^{\infty}xdF(x)$ e $\int_{-\infty}^{0}xdF(x)$, logo $X$ é integrável se $E(|X|) < \infty$.

## Propriedades da esperança

- $\mathbf{E_{1}}$: Se $X = c$, com $c$ uma constante, $E(X) = c$;
- $\mathbf{E_{2}}$**(monotonia)**: Se $X$ e $Y$ são variáveis aleatórias, com $X \le Y \Rightarrow E(X) \le E(Y)$, caso ambas as esperanças estejam bem definidas;

::: {.proof name="Prova"}

Seja $z$ um valor fixo. Se $Y \le z \Rightarrow X \le z$, logo $[Y \le z] \subseteq [X \le z]$, assim:

\begin{align*}
P(Y \le z) &\le P(X \le z) \\
F_{Y}(z) &\le F_{X}(z) \Longleftrightarrow 1 - F_{Y}(z) \ge 1 - F_{X}(z)
\end{align*}

E pela proposição \@ref(prp:separespe), temos que:

\begin{align*}
E(Y) = \int_{0}^{\infty}\big{[}1 - F_{Y}(z)\big{]}dz - \int_{-\infty}^{0}F_{Y}(z)dz &\ge \int_{0}^{\infty}\big{[}1 - F_{X}(z)\big{]}dz - \int_{-\infty}^{0}F_{X}(z)dz = E(X) \\
E(Y) &\ge E(X)
\end{align*}

:::

- $\mathbf{E_{3}}$**(linearidade)**:
  - **(i)** Se $E(X)$ é bem definida, $a,b \in \mathbb{R}$, então $E(aX + b) = aE(X) + b$;
  - **(ii)** $E(aX + bY) = aE(X) + bE(Y)$, caso o termo $aE(X) + bE(Y)$ esteja bem definido;
  - Note que se $E(X) = \infty \Rightarrow E(X - X) \neq E(X) - E(X)$.

::: {.proof name="Prova"}

Quando $a = 0; E(aX + b) = E(b) = b = 0E(X) + b$.

Quando $a > 0, b > 0; F_{aX + b}(x) = P(aX+b \le x) = P\left(X \le \frac{x-b}{a}\right) = F_{X}\left(\frac{x-b}{a}\right)$. Logo:

\begin{align*}
E(aX + b) &= \int_{0}^{\infty}\big{[}1 - F_{aX + b}(x)\big{]}dx - \int_{-\infty}^{0}F_{aX + b}(x)dx \\
&= \int_{0}^{\infty}\left[1 - F_{X}\left(\frac{x-b}{a}\right)\right]dx - \int_{-\infty}^{0}F_{X}\left(\frac{x-b}{a}\right)dx
\end{align*}

Tome $y = \frac{x-b}{a} \Rightarrow dy = \frac{1}{a}dx$. Então:

\begin{align*}
E(aX+b) &= \int_{-b/a}^{\infty}a\big{[}1 - F_{X}(y)\big{]}dy - \int_{-\infty}^{-b/a}aF_{X}(y)dy \\
&= a\left\{\int_{-b/a}^{\infty}\big{[}1 - F_{X}(y)\big{]}dy - \int_{-\infty}^{-b/a}F_{X}(y)dy\right\} \\
&= a\int_{0}^{\infty}\big{[}1 - F_{X}(y)\big{]}dy - a\int_{-\infty}^{0}F_{X}(y)dy + a\int_{-b/a}^{0}\big{[}1 - F_{X}(y)\big{]}dy + a\int_{-b/a}^{0}F_{X}(y)dy \\
&= aE(X) + a \int_{-b/a}^{0}dy \\
&= aE(X) + a \frac{b}{a} \\
&= aE(X) + b
\end{align*}

:::

- $\mathbf{E_{4}}$**(Desigualdade de Jansen)**: Seja $\varphi$ uma função convexa, definida na reta, com $X$ integrável, então:

\begin{equation}
E(\varphi(X)) \ge \varphi(E(X))
(\#eq:desigjansen)
\end{equation}

**Nota**: Caso $\varphi$ seja côncava:

\begin{equation*}
E(\varphi(X)) \le \varphi(E(X))
\end{equation*}

::: {.proof name="Prova para convexa"}

Tome $x_{0}$ e $\varphi(x_{0})$. Então existe uma reta $L$ tal que $L$ passe por $\varphi(x_{0})$ e $\varphi$ fica por cima de $L$. Logo temos a seguinte equação da reta:

\begin{equation*}
L(x) = \varphi(x_{0}) + \lambda(x - x_{0})
\end{equation*}

Onde $\lambda$ é alguma constante apropriada. Então para todo $x$ temos:

\begin{align*}
\varphi(x) &\ge L(x) = \varphi(x_{0}) + \lambda(x - x_{0}) \\
&\big{\Downarrow} \;\mathbf{E_{2}} \\
E(\varphi(x)) &\ge E(L(x)) \stackrel{\mathbf{E_{1},E_{3}}}{=} \varphi(x_{0}) + \lambda\left[E(x) - x_{0}\right]
\end{align*}

Que vale para $x_{0} = E(x)$, de modo que $E(\varphi(x)) \ge \varphi(E(x)) + \lambda\left[E(x) - E(x)\right]$, então:

\begin{equation*}
E(\varphi(x)) \ge \varphi(E(x))
\end{equation*}

A prova para funções côncavas segue a mesma metodologia, com a inversão da desigualdade.

:::

### Critério de integrabilidade

Suponha que $X$ é uma variável aleatória dominada por $Y$ (ou seja, $X \le Y$), sendo $Y$ uma variável aleatória integrável. $X$ é integrável? Temos que:

\begin{equation*}
X \le Y \Rightarrow E(X) \le E(Y)
\end{equation*}

Se $X$ e $Y$ são tais que $Y \ge 0$ e $Y$ é integrável e $|X| \le Y \Rightarrow 0 \le |X| \le Y$, e como consequência:

\begin{equation*}
0 \le E(X) \le E(Y) < \infty \Longrightarrow X \text{ é integrável}
\end{equation*}

De maneira similar, seja $X$ uma variável aleatória qualquer. Então:

\begin{equation*}
\sum_{n=1}^{\infty}P(|X| \ge n) \le E(|X|) \le 1 + \sum_{n=1}^{\infty}P(|X| \ge n)
\end{equation*}

Assim, $X$ é integrável se e somente se $\sum_{n=1}^{\infty}P(|X| \ge n) < \infty$.

::: {.proof name="Prova"}

Seja $x \ge 0$. Tome $[x]$ como a parte inteira de $x$. Então $[|x|] = k$ se $k \le |x| < k+1$. Então:

\begin{align*}
0 \le [|x|] &\le |x| \le [|x|] + 1 \\
&\Downarrow \; \mathbf{E_{2},E_{3}} \\
0 \le E([|x|]) &\le E(|x|) \le E([|x|]) + 1
\end{align*}

Pelo corolário \@ref(cor:esppositiva), como $[|x|]$ é discreta e não-negativa, temos que:

\begin{align*}
E([|x|]) &= \sum_{n=1}^{\infty}P([|x|] \ge n) \\
&= \sum_{n=1}^{\infty}P(|x| \ge n) \le E(|x|) \le \sum_{n=1}^{\infty}P(|x| \ge n) + 1
\end{align*}

:::

### Casos de interesse

**a) (Consistência absoluta)** $\varphi(X) = |X|$:

\begin{equation*}
E(|X|) \ge |E(X)|
\end{equation*}

**b) (Consistência quadrática)** $\varphi(X) = X^{2}$:

\begin{equation*}
E(X^{2}) \ge [E(X)]^{2}
\end{equation*}

**c) (Consistência absoluta de ordem p)** $\varphi(X) = |X|^{p}, p \ge 1$:

\begin{equation*}
E(|X|^{p}) \ge |E(X)|^{p}
\end{equation*}

**Nota**: $\varphi$ só precisa ser convexa (ou côncava) em uma região de probabilidade 1. Por exemplo, se $X$ é uma variável aleatória, tal que $P(X > 0) = 1$, ou o suporte da distribuição de $X$ é $(0, \infty), \varphi(X) = \frac{1}{X}$ é convexa em $(0,\infty) \Rightarrow E\left(\frac{1}{X}\right) \ge \frac{1}{E(X)}$. De modo análogo, se $P(X > 0) = 1$ e $\varphi(X) = \ln(X), \varphi$ é côncava em $(0,\infty)$ logo $E(\ln(X)) \le \ln(E(X))$.

## Esperança de funções de variáveis aleatórias

Seja $X$ uma variável aleatória, $\varphi$ uma função mensurável e $Y = \varphi(X)$. Assim, $Y$ é uma variável aleatória, cuja esperança é $E(Y) = \int ydF_{\varphi(X)}(y) = \int_{0}^{\infty}[1 - F_{\varphi(X)}(y)]dy - \int_{-\infty}^{0}F_{\varphi(X)}(y)dy$.

::: {.theorem}

Se $X$ é uma variável aleatória e $\varphi$ uma função mensurável, com $Y = \varphi(X)$:

\begin{equation*}
E(Y) = E(\varphi(X)) = \int\varphi(x)dF_{X}(x)
\end{equation*}

:::

::: {.proof name="Prova para caso $\varphi(x) = x^{k}$"}

Note que a prova já foi feita para $\varphi(x) = |x|$. Vejamos que a prova é válida para $\varphi(x) = x^{k}$, com $k = 1,2,\ldots$, em 2 casos: $k$ par e $k$ ímpar:

**$\mathbf{k}$ par:**

\begin{align*}
E(X^{k}) &= \int_{0}^{\infty}P\left(X^{k} > t\right)dt \\
&= \int_{0}^{\infty}P\left(X > \sqrt[k]{t}\right)dt + \int_{0}^{\infty}P\left(X < - \sqrt[k]{t}\right)dt \\
&= \int_{0}^{\infty}\left[1 - F_{X}\left(\sqrt[k]{t}\right)\right]dt + \int_{0}^{\infty}F_{X}\left(-\sqrt[k]{t}^{\;-}\right)dt
\end{align*}

Apliquemos as seguintes mudanças de variáveis: $s = t^{\frac{1}{k}}, ds = \frac{1}{k}t^{\frac{1}{k} - 1}dt, dt = \frac{(ds)ks^{k}}{s}, u = -s, du = -ds$:

\begin{align*}
E(X^{k}) &= \int_{0}^{\infty}\left[1-F_{X}(s)\right]ks^{k-1}ds + \int_{0}^{\infty}F_{X}(-s)ks^{k-1}ds \\
&= \int_{0}^{\infty}\left[1-F_{X}(s)\right]ks^{k-1}ds - \int_{-\infty}^{0}F_{X}(u)ku^{k-1}du \\
&= k\left\{\int_{0}^{\infty}[1 - F_{X}(s)]s^{k-1}ds - \int_{-\infty}^{0}F_{X}(u)u^{k-1}du\right\}
\end{align*}

Agora, mostremos que $E(X^{k}) = \int x^{k}dF_{X}(x)$:

\begin{align*}
\int_{-\infty}^{\infty}x^{k}dF_{X}(x) &\stackrel{Def}{=} \int_{0}^{\infty}\left[1 - F_{X}(x)\right]d(x^{k}) - \int_{-\infty}^{0}F_{X}(x)d(x^{k}) \\
&= k\left\{\int_{0}^{\infty}[1 - F_{X}(x)]x^{k-1}dx - \int_{-\infty}^{0}F_{X}(x)x^{k-1}dx\right\} \\
&= E(X^{k})
\end{align*}

:::

**Nota**: A propriedade é também válida para polinômios, visto que a esperança opera de maneira linear.

::: {.example}

Seja $X \sim Exp(\lambda)$, vimos que $E(X) = \frac{1}{\lambda}$:

Calcular $E(X^{2})$:

\begin{align*}
E(X^{2}) = 2\int_{0}^{\infty}xe^{-\lambda x}dx &= \frac{2}{\lambda}\int_{0}^{\infty}x\lambda e^{-\lambda x}dx \\
&= \frac{2}{\lambda}E(X) = \frac{2}{\lambda^{2}}
\end{align*}

Calcular $E(X^{3})$:

\begin{align*}
E(X^{3}) = 3\int_{0}^{\infty}x^{2}e^{-\lambda x}dx &= \frac{3}{\lambda}\int_{0}^{\infty}x^{2}\lambda e^{-\lambda x}dx \\
&= \frac{3}{\lambda}E(X^{2}) = \frac{3}{\lambda^{3}}
\end{align*}

De modo que podemos observar o padrão emergente, e definir $E(X^{k}) = \frac{k!}{\lambda^{k}}$.

:::

## Momentos de uma variável aleatória

**a)** $E\left(\left[X - b\right]^{k}\right)$: $k-$ésimo momento de $X$ em torno de $b$;

**b)** $E\left(X^{k}\right)$: $k-$ésimo momento em torno de 0;

**c)** Se em **(a)**, $b = E(X)$, o momento é central;

**d)** $t>0,E\left(|X|^{t}\right)$: $t-$ésimo momento absoluto de $X$.

::: {.definition #Variancia name="Variância de uma variável aleatória"}

\begin{equation*}
Var(X) = E\left\{(X - E(X))^{2}\right\} \Longleftrightarrow Var(X) = E\left(X^{2}\right) - \big{(}E(X)\big{)}^{2}
\end{equation*}

:::

::: {.proposition}

Se $X$ é uma variável aleatória, $f(t) = \left[E(|X|^{t})\right]^{\frac{1}{t}}$ é não-decrescente em $t, t >0$.

:::

::: {.proof name="Prova"}

Devemos provar que, se $0 < s < t, f(s) \le f(t)$ (ou $\left\{E(|X|^{s})\right\}^{\frac{1}{s}} \le \left\{E(|X|^{t})\right\}^{\frac{1}{t}}$). Para tanto, consideremos dois casos: **a)** $E(|X|^{s})<\infty$, **b)** $E(|X|^{s})=\infty$:

**a)** Defina $\varphi(y) = |y|^{\frac{t}{s}}$ (caso $\frac{t}{s} > 1, \varphi$ será convexa). Pela Desigualdade de Jansen:

\begin{align*}
E(\varphi(Y)) &\ge \varphi(E(Y)) \\
E\left(|Y|^{\frac{t}{s}}\right) &\ge \left|E(Y)\right|^{\frac{t}{s}}
\end{align*}

Tome $Y = |X|^{s}$. Substituindo temos:

\begin{align*}
E\left((|X|^{s})^{\frac{t}{s}}\right) &\ge \left|E(|X|^{s})\right|^{\frac{t}{s}} \\
E\left(|X|^{t}\right) &\ge \left\{E(|X|^{s})\right\}^{\frac{t}{s}} \\
\left\{E(|X|^{t})\right\}^{\frac{1}{t}} &\ge \left\{E(|X|^{s})\right\}^{\frac{1}{s}}
\end{align*}

**b)** Como $t > s > 0$, sabemos que $|X|^{s} \le 1 + |X|^{t}$. Como $E(|X|^{s}) = \infty$, então:

\begin{equation*}
\infty = E(|X|^{s}) \le 1 + E(|X|^{t}) = \infty
\end{equation*}

:::

::: {.corollary}

Se $E(|X|^{t}) < \infty \; \forall t \in (0,\infty) \Rightarrow E(|X|^{s}) < \infty \; \forall s$, com $0 < s < t$.

:::

### Propriedades


- $\mathbf{E_{5}}$: Se $X = c$, com $c$ uma constante, $Var(X) = 0$;
- $\mathbf{E_{6}}$: $Var(X+b) = Var(X), Var(aX + b) = a^{2}Var(X)$, com $a,b \in \mathbb{R}$;

::: {.proof name="Prova"}

\begin{align*}
Var(aX + b) &= E\left\{\left[aX + b - E(aX + b)\right]^{2}\right\} \\
&= E\left\{\left[aX + b - aE(X) - b\right]^{2}\right\} \\
&= E\left\{a^{2}\left[X - E(X)\right]^{2}\right\} \\
&= a^{2}E\left\{\left[X - E(X)\right]^{2}\right\} = a^{2}Var(X)
\end{align*}

:::

- $\mathbf{E_{7}}$**(Desigualdade de Tchebychev)**: Seja $X$ uma variável aleatória, com $X \ge 0$. Para todo $\lambda > 0$:

\begin{equation}
P(X \ge \lambda) \le \frac{E(X)}{\lambda}
(\#eq:desigcheby)
\end{equation}

::: {.proof name="Prova"}

Seja $Y = I_{[X \ge \lambda]}\lambda = \begin{cases}\lambda &,X \ge \lambda \\ 0 &,c.c.\end{cases}$. Por definição, $0 \le Y \le X \Rightarrow E(Y) \le E(X)$ e $E(Y) = \lambda P(X \ge \lambda)$, de modo que:

\begin{equation*}
\lambda P(X \ge \lambda) \le E(X) \Leftrightarrow P(X \ge \lambda) \le \frac{E(X)}{\lambda}
\end{equation*}

:::

### Consequências

**a)** Para todo $\lambda>0$:

\begin{equation*}
P(|X - E(X)| \ge \lambda) \le \frac{Var(X)}{\lambda^{2}}
\end{equation*}

**b) (Desigualdade de Markov)** Seja $X$ uma variável aleatória, para todo $t$:

\begin{equation}
P(|X| \ge \lambda) \le \frac{E\left(|X|^{t}\right)}{\lambda^{t}}
(\#eq:desigmarkov)
\end{equation}

**c)** Se $Z$ é uma variável aleatória, com $Z \ge 0$ e $E(Z) = 0$:

\begin{equation*}
P(Z=0) = 1 \;\;\;\;(\text{i.e., }Z=0\text{ quase certamente})
\end{equation*}

::: {.proof name="Provas"}

**a)** Se $Y = [X - E(X)]^{2}$, aplicamos $\mathbf{E_{7}}$ usando $\lambda^{2}: P(Y \ge \lambda^{2}) \le \frac{E(Y)}{\lambda^{2}}$. Note que $E(Y) = E([X - E(X)]^{2}) = Var(X)$. Logo:

\begin{equation*}
P\left(|X - E(X)| \ge \lambda\right) = P(|X - E(X)|^{2} \ge \lambda^{2}) = P(Y \ge \lambda^{2}) \le \frac{E(Y)}{\lambda^{2}} = \frac{Var(X)}{\lambda^{2}}
\end{equation*}

**b)** Seja $Y = |X|^{t}$, aplicamos $\mathbf{E_{7}}$ a $Y$ e $\lambda^{t}: P(Y \ge \lambda^{t}) \le \frac{E(Y)}{\lambda^{t}}$. Note que $E(Y) = E(|X|^{t})$ e que $P(Y \ge \lambda^{t}) = P(|X|^{t} \ge \lambda^{t}) = P(|X| > \lambda)$. Logo:

\begin{equation*}
P(|X| \ge \lambda) \le \frac{E(|X|^{t})}{\lambda^{t}}
\end{equation*}

**c)** $Z = 0$ quase certamente, usamos $\mathbf{E_{7}}$ na variável $Z$ e em $\lambda = \frac{1}{n}$, então:

\begin{equation*}
P\left(Z \ge \frac{1}{n}\right) \le E(Z).n \stackrel{Hip}{=}0
\end{equation*}

Temos que $[Z > 0] = \bigcup_{n}\left[Z \ge \frac{1}{n}\right]$, de modo que:

\begin{equation*}
P(Z > 0) = P\left(\bigcup_{n}\left[Z \ge \frac{1}{n}\right]\right) = \lim_{n \to \infty}P\left(Z \ge \frac{1}{n}\right) = 0 \Rightarrow P(Z = 0) = 1 - P(Z > 0) = 1
\end{equation*}

:::
